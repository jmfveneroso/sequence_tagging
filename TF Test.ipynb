{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hello World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'HelloWorld'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "h = tf.constant('Hello')\n",
    "w = tf.constant('World')\n",
    "hw = h + w\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    ans = sess.run(hw)    \n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Regression on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "DATA_DIR = '/tmp/data'\n",
    "NUM_STEPS = 1000\n",
    "MINIBATCH_SIZE = 100\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "\n",
    "y_true = tf.placeholder(tf.float32, [None, 10])\n",
    "y_pred = tf.matmul(x, W)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred, labels=y_true))\n",
    "\n",
    "gd_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "correct_mask = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y_true, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_mask, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for _ in range(NUM_STEPS):\n",
    "        batch_xs, batch_ys = data.train.next_batch(MINIBATCH_SIZE)\n",
    "        sess.run(gd_step, feed_dict={x: batch_xs, y_true: batch_ys})\n",
    "        \n",
    "    ans = sess.run(accuracy, feed_dict={x: data.test.images, y_true: data.test.labels})    \n",
    "    \n",
    "print(\"Accuracy: {:.4}%\".format(ans*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(5)\n",
    "b = tf.constant(2)\n",
    "c = tf.constant(3)\n",
    "d = tf.multiply(a, b)\n",
    "e = tf.add(c, b)\n",
    "f = tf.subtract(d, e)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    fetches = [a, b, c, d, e, f]\n",
    "    outs = sess.run(fetches)\n",
    "\n",
    "print(\"outs: {}\".format(outs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(1)\n",
    "b = tf.constant(2)\n",
    "c = tf.multiply(a, b)\n",
    "d = tf.add(a, b)\n",
    "e = tf.subtract(c, d)\n",
    "f = tf.add(c, d)\n",
    "g = tf.divide(e, f)\n",
    "\n",
    "sess = tf.Session()\n",
    "outs = sess.run(g)\n",
    "\n",
    "print('Outs: {}'.format(outs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(1.)\n",
    "b = tf.constant(2.)\n",
    "c = tf.multiply(a, b)\n",
    "d = tf.sin(c)\n",
    "e = tf.divide(d, b)\n",
    "\n",
    "sess = tf.Session()\n",
    "outs = sess.run(e)\n",
    "print('Outs: {}'.format(outs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "g1 = tf.get_default_graph()\n",
    "g2 = tf.Graph()\n",
    "\n",
    "with g2.as_default():\n",
    "    print(g1 is tf.get_default_graph())\n",
    "a = tf.constant(5)\n",
    "\n",
    "print(a.graph is g)\n",
    "print(a.graph is tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant([1, 2, 3], name='x', dtype=tf.float64)\n",
    "print(x)\n",
    "x = tf.cast(x, tf.int64, name='cast_x')\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "c = tf.constant([[1,2,3],[4,5,6]])\n",
    "print('c: {}'.format(c.get_shape()))\n",
    "\n",
    "c = tf.constant(np.array([\n",
    "    [[1,2,3],[4,5,6]],\n",
    "    [[1,1,1],[2,2,2]]\n",
    "]))\n",
    "print('c: {}'.format(c.get_shape()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "c = tf.linspace(0.0, 4.0, 5)\n",
    "print('c: {}'.format(c.eval()))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "(3, 1)\n",
      "matmul result: [[ 4]\n",
      " [10]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "A = tf.constant([[1,2,3],[4,5,6]])\n",
    "print(A.get_shape())\n",
    "\n",
    "x = tf.constant([1,0,1])\n",
    "x = tf.expand_dims(x, 1)\n",
    "print(x.get_shape())\n",
    "\n",
    "b = tf.matmul(A, x)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "print('matmul result: {}'.format(b.eval()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre run: <tf.Variable 'var_5:0' shape=(1, 5) dtype=float32_ref>\n",
      "post run: [[ 0.07406364 -0.37498748  0.24438557  1.5451785  -0.07271151]]\n"
     ]
    }
   ],
   "source": [
    "init_val = tf.random_normal((1,5),0,1)\n",
    "var = tf.Variable(init_val, name='var')\n",
    "print('pre run: {}'.format(var))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    post_var = sess.run(var)\n",
    "    \n",
    "print('post run: {}'.format(post_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outs = 3.251608371734619\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_data = np.random.randn(5,10)\n",
    "w_data = np.random.randn(10,1)\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    x = tf.placeholder(tf.float32,shape=(5,10))\n",
    "    w = tf.placeholder(tf.float32,shape=(10,1))\n",
    "    b = tf.fill((5,1),-1.)\n",
    "    xw = tf.matmul(x, w)\n",
    "    xwb = xw + b\n",
    "    s = tf.reduce_max(xwb)\n",
    "    with tf.Session() as sess:\n",
    "        outs = sess.run(s, feed_dict={x: x_data, w: w_data})\n",
    "        \n",
    "print('outs = {}'.format(outs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [array([[0.29944515, 0.52722275, 0.10362744]], dtype=float32), -0.20495152]\n",
      "5 [array([[0.29500726, 0.5001984 , 0.10093738]], dtype=float32), -0.20034765]\n",
      "10 [array([[0.29500726, 0.5001984 , 0.10093737]], dtype=float32), -0.20034763]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate data.\n",
    "x_data = np.random.randn(2000,3)\n",
    "w_real = [0.3,0.5,0.1]\n",
    "b_real = -0.2\n",
    "\n",
    "noise = np.random.randn(1,2000)*0.1\n",
    "y_data = np.matmul(w_real, x_data.T) + b_real + noise\n",
    "\n",
    "\n",
    "# Model.\n",
    "NUM_STEPS = 10\n",
    "\n",
    "g = tf.Graph()\n",
    "wb_ = []\n",
    "with g.as_default():\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "    y_true = tf.placeholder(tf.float32, shape=None)\n",
    "    \n",
    "    with tf.name_scope('inference') as scope:\n",
    "        w = tf.Variable([[0,0,0]],dtype=tf.float32,name='weights')\n",
    "        b = tf.Variable(0,dtype=tf.float32,name='bias')\n",
    "        y_pred = tf.matmul(w, tf.transpose(x)) + b\n",
    "\n",
    "    with tf.name_scope('loss') as scope:\n",
    "        loss = tf.reduce_mean(tf.square(y_true-y_pred))\n",
    "\n",
    "    with tf.name_scope('train') as scope:\n",
    "        learning_rate = 0.5\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        train = optimizer.minimize(loss)\n",
    "        \n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for step in range(NUM_STEPS):\n",
    "            sess.run(train,{x:x_data,y_true:y_data})\n",
    "            if (step % 5 == 0):\n",
    "                print(step, sess.run([w,b]))\n",
    "                wb_.append(sess.run([w,b]))\n",
    "        \n",
    "        print(10, sess.run([w,b]))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [array([[0.00928201, 0.01450536, 0.00252127]], dtype=float32), -0.02111341]\n",
      "5 [array([[0.05440196, 0.08505421, 0.01477653]], dtype=float32), -0.1240038]\n",
      "10 [array([[0.09671815, 0.15131567, 0.02625452]], dtype=float32), -0.2215482]\n",
      "15 [array([[0.13573062, 0.21252505, 0.03680407]], dtype=float32), -0.31304452]\n",
      "20 [array([[0.17134641, 0.26853004, 0.04639117]], dtype=float32), -0.39832264]\n",
      "25 [array([[0.20373096, 0.31957093, 0.05505797]], dtype=float32), -0.47757095]\n",
      "30 [array([[0.23317596, 0.36608312, 0.06288499]], dtype=float32), -0.5511742]\n",
      "35 [array([[0.26001254, 0.4085659 , 0.06996584]], dtype=float32), -0.6195994]\n",
      "40 [array([[0.28456405, 0.4475101 , 0.07639296]], dtype=float32), -0.6833277]\n",
      "45 [array([[0.30712482, 0.48336455, 0.08225114]], dtype=float32), -0.7428184]\n",
      "10 [array([[0.323915  , 0.51009125, 0.08657864]], dtype=float32), -0.7876448]\n"
     ]
    }
   ],
   "source": [
    "N = 20000\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "x_data = np.random.randn(N,3)\n",
    "w_real = [0.3,0.5,0.1]\n",
    "b_real = -0.2\n",
    "\n",
    "wxb = np.matmul(w_real,x_data.T) + b_real\n",
    "\n",
    "y_data_pre_noise = sigmoid(wxb)\n",
    "y_data = np.random.binomial(1,y_data_pre_noise)\n",
    "\n",
    "\n",
    "# Model.\n",
    "NUM_STEPS = 50\n",
    "\n",
    "g = tf.Graph()\n",
    "wb_ = []\n",
    "with g.as_default():\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "    y_true = tf.placeholder(tf.float32, shape=None)\n",
    "    \n",
    "    with tf.name_scope('inference') as scope:\n",
    "        w = tf.Variable([[0,0,0]],dtype=tf.float32,name='weights')\n",
    "        b = tf.Variable(0,dtype=tf.float32,name='bias')\n",
    "        y_pred = tf.matmul(w, tf.transpose(x)) + b\n",
    "        y_pred = tf.sigmoid(y_pred)\n",
    "        \n",
    "    with tf.name_scope('loss') as scope:\n",
    "        loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true,logits=y_pred)\n",
    "        loss = tf.reduce_mean(loss)        \n",
    "        \n",
    "    with tf.name_scope('train') as scope:\n",
    "        learning_rate = 0.5\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        train = optimizer.minimize(loss)\n",
    "        \n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for step in range(NUM_STEPS):\n",
    "            sess.run(train,{x:x_data,y_true:y_data})\n",
    "            if (step % 5 == 0):\n",
    "                print(step, sess.run([w,b]))\n",
    "                wb_.append(sess.run([w,b]))\n",
    "        \n",
    "        print(10, sess.run([w,b]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "    \n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1,1,1,1],padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "def conv_layer(input, shape):\n",
    "    W = weight_variable(shape)\n",
    "    b = bias_variable([shape[3]])\n",
    "    return tf.nn.relu(conv2d(input, W) + b)\n",
    "\n",
    "def full_layer(input, size):\n",
    "    in_size = int(input.get_shape()[1])\n",
    "    W = weight_variable([in_size, size])\n",
    "    b = bias_variable([size])\n",
    "    return tf.matmul(input, W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "Step 0 accuracy 0.05999999865889549\n",
      "Step 100 accuracy 0.9200000166893005\n",
      "Step 200 accuracy 0.9200000166893005\n",
      "Step 300 accuracy 0.9200000166893005\n",
      "Step 400 accuracy 0.9599999785423279\n",
      "Step 500 accuracy 0.9599999785423279\n",
      "Step 600 accuracy 0.9800000190734863\n",
      "Step 700 accuracy 0.9399999976158142\n",
      "Step 800 accuracy 1.0\n",
      "Step 900 accuracy 0.9800000190734863\n",
      "Step 1000 accuracy 0.9599999785423279\n",
      "Step 1100 accuracy 0.9800000190734863\n",
      "Step 1200 accuracy 0.9599999785423279\n",
      "Step 1300 accuracy 0.8999999761581421\n",
      "Step 1400 accuracy 0.9599999785423279\n",
      "Step 1500 accuracy 0.9800000190734863\n",
      "Step 1600 accuracy 1.0\n",
      "Step 1700 accuracy 1.0\n",
      "Step 1800 accuracy 0.9599999785423279\n",
      "Step 1900 accuracy 1.0\n",
      "Step 2000 accuracy 0.9800000190734863\n",
      "Step 2100 accuracy 1.0\n",
      "Step 2200 accuracy 1.0\n",
      "Step 2300 accuracy 1.0\n",
      "Step 2400 accuracy 1.0\n",
      "Step 2500 accuracy 1.0\n",
      "Step 2600 accuracy 1.0\n",
      "Step 2700 accuracy 1.0\n",
      "Step 2800 accuracy 1.0\n",
      "Step 2900 accuracy 0.9800000190734863\n",
      "Step 3000 accuracy 0.9599999785423279\n",
      "Step 3100 accuracy 0.9599999785423279\n",
      "Step 3200 accuracy 0.9800000190734863\n",
      "Step 3300 accuracy 1.0\n",
      "Step 3400 accuracy 1.0\n",
      "Step 3500 accuracy 1.0\n",
      "Step 3600 accuracy 0.9800000190734863\n",
      "Step 3700 accuracy 0.9800000190734863\n",
      "Step 3800 accuracy 1.0\n",
      "Step 3900 accuracy 0.9800000190734863\n",
      "Step 4000 accuracy 1.0\n",
      "Step 4100 accuracy 0.9800000190734863\n",
      "Step 4200 accuracy 0.9800000190734863\n",
      "Step 4300 accuracy 0.9599999785423279\n",
      "Step 4400 accuracy 0.9800000190734863\n",
      "Step 4500 accuracy 1.0\n",
      "Step 4600 accuracy 1.0\n",
      "Step 4700 accuracy 1.0\n",
      "Step 4800 accuracy 0.9800000190734863\n",
      "Step 4900 accuracy 0.9800000190734863\n",
      "Test accuracy: 0.9902000427246094\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "DATA_DIR = '/tmp/data'\n",
    "STEPS = 5000\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "conv1 = conv_layer(x_image, shape=[5, 5, 1, 32])\n",
    "conv1_pool = max_pool_2x2(conv1)\n",
    "\n",
    "conv2 = conv_layer(conv1_pool, shape=[5, 5, 32, 64])\n",
    "conv2_pool = max_pool_2x2(conv2)\n",
    "\n",
    "conv2_flat = tf.reshape(conv2_pool, [-1, 7*7*64])\n",
    "full_1 = tf.nn.relu(full_layer(conv2_flat, 1024))\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "full1_drop = tf.nn.dropout(full_1, keep_prob=keep_prob)\n",
    "\n",
    "y_conv = full_layer(full1_drop, 10)\n",
    "\n",
    "mnist = input_data.read_data_sets(DATA_DIR, one_hot=True)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_conv, labels=y_))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())    \n",
    "    \n",
    "    for i in range(STEPS):\n",
    "        batch = mnist.train.next_batch(50)\n",
    "        if i % 100 == 0:\n",
    "            train_accuracy = sess.run(accuracy, feed_dict={\n",
    "                x: batch[0],\n",
    "                y_: batch[1],\n",
    "                keep_prob: 1.0\n",
    "            })\n",
    "            print(\"Step {} accuracy {}\".format(i, train_accuracy))\n",
    "           \n",
    "        sess.run(train_step, feed_dict={\n",
    "            x: batch[0],\n",
    "            y_: batch[1],\n",
    "            keep_prob: 0.5\n",
    "        })\n",
    "        \n",
    "    X = mnist.test.images.reshape(10, 1000, 784)\n",
    "    Y = mnist.test.labels.reshape(10, 1000, 10)    \n",
    "    test_accuracy = np.mean([\n",
    "        sess.run(accuracy, feed_dict={x: X[i], y_: Y[i], keep_prob: 1.0})\n",
    "        for i in range(10)\n",
    "    ])\n",
    "    \n",
    "    print(\"Test accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train images: 50000\n",
      "Number of train labels: 50000\n",
      "Number of test images: 10000\n",
      "Number of test labels: 10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import _pickle\n",
    "# %matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DATA_PATH = './cifar-10-batches-py'\n",
    "        \n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(os.path.join(DATA_PATH, file), 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict        \n",
    "        \n",
    "def one_hot(vec, vals=10):\n",
    "    n = len(vec)\n",
    "    out = np.zeros((n, vals))\n",
    "    out[range(n), vec] = 1\n",
    "    return out\n",
    "\n",
    "class CifarLoader(object):\n",
    "    def __init__(self, source_files):\n",
    "        self._source = source_files\n",
    "        self._i = 0\n",
    "        self.images = None\n",
    "        self.labels = None\n",
    "        \n",
    "    def load(self):\n",
    "        data = [unpickle(f) for f in self._source]\n",
    "        images = np.vstack([d[b'data'] for d in data])\n",
    "        n = len(images)\n",
    "        self.images = images.reshape(n, 3, 32, 32).transpose(0, 2, 3, 1).astype(float) / 255\n",
    "        self.labels = one_hot(np.hstack(d[b'labels'] for d in data), 10)\n",
    "        return self\n",
    "    \n",
    "    def next_batch(self, batch_size):\n",
    "        x = self.images[self._i:self._i+batch_size]\n",
    "        y = self.labels[self._i:self._i+batch_size]\n",
    "        self._i = (self._i + batch_size) % len(self.images)\n",
    "        return x, y\n",
    "\n",
    "class CifarDataManager(object):\n",
    "    def __init__(self):\n",
    "        self.train = CifarLoader([\n",
    "            \"data_batch_{}\".format(i)\n",
    "            for i in range(1, 6)\n",
    "        ]).load()        \n",
    "        self.test = CifarLoader([\"test_batch\"]).load()\n",
    "        \n",
    "def display_cifar(images, size):\n",
    "    n = len(images)\n",
    "    plt.figure()\n",
    "    plt.gca().set_axis_off()\n",
    "    im = np.vstack([np.hstack([images[np.random.choice(n)] for i in range(size)]) for i in range(size)])\n",
    "    plt.imshow(im)\n",
    "    plt.show()\n",
    "    \n",
    "d = CifarDataManager()\n",
    "print(\"Number of train images: {}\".format(len(d.train.images)))\n",
    "print(\"Number of train labels: {}\".format(len(d.train.labels)))\n",
    "print(\"Number of test images: {}\".format(len(d.test.images)))\n",
    "print(\"Number of test labels: {}\".format(len(d.test.labels)))\n",
    "\n",
    "images = d.train.images\n",
    "display_cifar(images, 10)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-00cd32528abc>:30: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Step 0 accuracy 0.11999999731779099\n",
      "Step 100 accuracy 0.3799999952316284\n",
      "Step 200 accuracy 0.41999998688697815\n",
      "Step 300 accuracy 0.3400000035762787\n",
      "Step 400 accuracy 0.4000000059604645\n",
      "Step 500 accuracy 0.5199999809265137\n",
      "Step 600 accuracy 0.5199999809265137\n",
      "Step 700 accuracy 0.5199999809265137\n",
      "Step 800 accuracy 0.6600000262260437\n",
      "Step 900 accuracy 0.699999988079071\n",
      "Step 1000 accuracy 0.6000000238418579\n",
      "Step 1100 accuracy 0.5\n",
      "Step 1200 accuracy 0.5400000214576721\n",
      "Step 1300 accuracy 0.6200000047683716\n",
      "Step 1400 accuracy 0.4399999976158142\n",
      "Step 1500 accuracy 0.6399999856948853\n",
      "Step 1600 accuracy 0.6800000071525574\n",
      "Step 1700 accuracy 0.6200000047683716\n",
      "Step 1800 accuracy 0.6600000262260437\n",
      "Step 1900 accuracy 0.8399999737739563\n",
      "Step 2000 accuracy 0.6600000262260437\n",
      "Step 2100 accuracy 0.6399999856948853\n",
      "Step 2200 accuracy 0.7200000286102295\n",
      "Step 2300 accuracy 0.6600000262260437\n",
      "Step 2400 accuracy 0.5400000214576721\n",
      "Step 2500 accuracy 0.7400000095367432\n",
      "Step 2600 accuracy 0.7799999713897705\n",
      "Step 2700 accuracy 0.6800000071525574\n",
      "Step 2800 accuracy 0.7799999713897705\n",
      "Step 2900 accuracy 0.8999999761581421\n",
      "Step 3000 accuracy 0.7599999904632568\n",
      "Step 3100 accuracy 0.7200000286102295\n",
      "Step 3200 accuracy 0.7400000095367432\n",
      "Step 3300 accuracy 0.7400000095367432\n",
      "Step 3400 accuracy 0.6399999856948853\n",
      "Step 3500 accuracy 0.800000011920929\n",
      "Step 3600 accuracy 0.7400000095367432\n",
      "Step 3700 accuracy 0.7799999713897705\n",
      "Step 3800 accuracy 0.800000011920929\n",
      "Step 3900 accuracy 0.8600000143051147\n",
      "Step 4000 accuracy 0.8399999737739563\n",
      "Step 4100 accuracy 0.7400000095367432\n",
      "Step 4200 accuracy 0.7599999904632568\n",
      "Step 4300 accuracy 0.699999988079071\n",
      "Step 4400 accuracy 0.699999988079071\n",
      "Step 4500 accuracy 0.800000011920929\n",
      "Step 4600 accuracy 0.7599999904632568\n",
      "Step 4700 accuracy 0.7400000095367432\n",
      "Step 4800 accuracy 0.8399999737739563\n",
      "Step 4900 accuracy 0.8999999761581421\n",
      "Test accuracy: 70.75%\n"
     ]
    }
   ],
   "source": [
    "cifar = CifarDataManager()\n",
    "\n",
    "DATA_DIR = '/tmp/data'\n",
    "STEPS = 5000\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "C1, C2, C3 = 30, 50, 80\n",
    "F1 = 500\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "conv1_1 = conv_layer(x, shape=[3, 3, 3, C1])\n",
    "conv1_pool = max_pool_2x2(conv1_1)\n",
    "\n",
    "conv2_1 = conv_layer(conv1_pool, shape=[3, 3, C1, C2])\n",
    "conv2_pool = max_pool_2x2(conv2_1)\n",
    "\n",
    "conv3_1 = conv_layer(conv2_pool, shape=[3, 3, C2, C3])\n",
    "conv3_pool = max_pool_2x2(conv3_1)\n",
    "conv3_flat = tf.reshape(conv3_pool, [-1, 4*4*C3])\n",
    "conv3_drop = tf.nn.dropout(conv3_flat, keep_prob=keep_prob)\n",
    "\n",
    "full_1 = tf.nn.relu(full_layer(conv3_flat, F1))\n",
    "full1_drop = tf.nn.dropout(full_1, keep_prob=keep_prob)\n",
    "\n",
    "y_conv = full_layer(full1_drop, 10)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_conv, labels=y_))\n",
    "train_step = tf.train.AdamOptimizer(1e-3).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "def test(sess):\n",
    "    X = cifar.test.images.reshape(10, 1000, 32, 32, 3)\n",
    "    Y = cifar.test.labels.reshape(10, 1000, 10)\n",
    "    acc = np.mean([\n",
    "        sess.run(accuracy, feed_dict={x:X[i], y_:Y[i], keep_prob: 1.0}) \n",
    "        for i in range(10)\n",
    "    ])\n",
    "    print(\"Test accuracy: {:.4}%\".format(acc * 100))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())    \n",
    "    \n",
    "    for i in range(STEPS):\n",
    "        batch = cifar.train.next_batch(BATCH_SIZE)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            train_accuracy = sess.run(accuracy, feed_dict={\n",
    "                x: batch[0],\n",
    "                y_: batch[1],\n",
    "                keep_prob: 1.0\n",
    "            })\n",
    "            print(\"Step {} accuracy {}\".format(i, train_accuracy))\n",
    "            \n",
    "        sess.run(train_step, feed_dict={\n",
    "            x: batch[0],\n",
    "            y_: batch[1],\n",
    "            keep_prob: 0.5\n",
    "        })\n",
    "        \n",
    "    test(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([b'T', b'Te', b'Ten', b'Tens', b'Tenso', b'Tensor', b'Tensor ',\n",
       "       b'Tensor F', b'Tensor Fl', b'Tensor Flo', b'Tensor Flow'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elems = np.array(['T','e','n','s','o','r',' ','F','l','o','w'])\n",
    "scan_sum = tf.scan(lambda a, x: a + x, elems)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(scan_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN for MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-3d3c5a09bd44>:4: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From <ipython-input-1-3d3c5a09bd44>:79: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Iter 0, Minibatch loss= 2.302967, Training accuracy= 10.15625\n",
      "Iter 1000, Minibatch loss= 1.276900, Training accuracy= 54.68750\n",
      "Iter 2000, Minibatch loss= 0.697088, Training accuracy= 76.56250\n",
      "Iter 3000, Minibatch loss= 0.446154, Training accuracy= 85.93750\n",
      "Iter 4000, Minibatch loss= 0.121886, Training accuracy= 98.43750\n",
      "Iter 5000, Minibatch loss= 0.109163, Training accuracy= 97.65625\n",
      "Iter 6000, Minibatch loss= 0.089136, Training accuracy= 97.65625\n",
      "Iter 7000, Minibatch loss= 0.185452, Training accuracy= 96.09375\n",
      "Iter 8000, Minibatch loss= 0.147291, Training accuracy= 97.65625\n",
      "Iter 9000, Minibatch loss= 0.070955, Training accuracy= 99.21875\n",
      "Test accuracy: 99.21875\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets('/tmp/data', one_hot=True)\n",
    "\n",
    "element_size = 28\n",
    "time_steps = 28\n",
    "num_classes = 10\n",
    "batch_size = 128\n",
    "hidden_layer_size = 128\n",
    "\n",
    "LOG_DIR = 'logs/RNN_with_summaries'\n",
    "\n",
    "_inputs = tf.placeholder(tf.float32, shape=[None, time_steps, element_size], name='inputs')\n",
    "y = tf.placeholder(tf.float32, shape=[None, num_classes], name='labels')\n",
    "\n",
    "def variable_summaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)\n",
    "                             \n",
    "with tf.name_scope('rnn_weights'):\n",
    "    Wx = tf.Variable(tf.zeros([element_size, hidden_layer_size]))\n",
    "    variable_summaries(Wx)\n",
    "     \n",
    "with tf.name_scope('rnn_weights'):\n",
    "    with tf.name_scope('W_x'):\n",
    "        Wx = tf.Variable(tf.zeros([element_size, hidden_layer_size]))\n",
    "        variable_summaries(Wx)\n",
    "    with tf.name_scope('W_h'):\n",
    "        Wh = tf.Variable(tf.zeros([hidden_layer_size, hidden_layer_size]))\n",
    "        variable_summaries(Wh)\n",
    "    with tf.name_scope('Bias'):\n",
    "        b_rnn = tf.Variable(tf.zeros([hidden_layer_size]))\n",
    "        variable_summaries(b_rnn)\n",
    "                         \n",
    "def rnn_step(previous_hidden_state, x):\n",
    "    current_hidden_state = tf.tanh(\n",
    "        tf.matmul(previous_hidden_state, Wh) +\n",
    "        tf.matmul(x, Wx) + b_rnn\n",
    "    )\n",
    "    return current_hidden_state\n",
    "\n",
    "processed_inputs = tf.transpose(_inputs, perm=[1,0,2])\n",
    "initial_hidden = tf.zeros([batch_size, hidden_layer_size])\n",
    "all_hidden_states = tf.scan(rnn_step, processed_inputs, initializer=initial_hidden, name='states')\n",
    "                             \n",
    "with tf.name_scope('linear_layer_weights') as scope:\n",
    "    with tf.name_scope('W_linear'):\n",
    "        Wl = tf.Variable(tf.truncated_normal([\n",
    "            hidden_layer_size,\n",
    "            num_classes\n",
    "        ], mean=0, stddev=.01))\n",
    "        variable_summaries(Wl)\n",
    "                             \n",
    "    with tf.name_scope('Bias_linear'):\n",
    "        bl = tf.Variable(tf.truncated_normal(\n",
    "            [num_classes],\n",
    "            mean=0, stddev=.01\n",
    "        ))\n",
    "        variable_summaries(bl)\n",
    "\n",
    "def get_linear_layer(hidden_state):\n",
    "    return tf.matmul(hidden_state, Wl) + bl\n",
    "\n",
    "with tf.name_scope('linear_layer_weights') as scope:\n",
    "    all_outputs = tf.map_fn(get_linear_layer, all_hidden_states)\n",
    "    output = all_outputs[-1]\n",
    "    tf.summary.histogram('outputs', output)\n",
    "\n",
    "with tf.name_scope('cross_entropy') as scope:\n",
    "    cross_entropy = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=y)\n",
    "    )\n",
    "    tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "                             \n",
    "with tf.name_scope('train') as scope:\n",
    "    train_step = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cross_entropy)\n",
    "    \n",
    "with tf.name_scope('accuracy') as scope:\n",
    "    correct_prediction = tf.equal(\n",
    "        tf.argmax(y, 1),\n",
    "        tf.argmax(output, 1)\n",
    "    )\n",
    "    accuracy = (tf.reduce_mean(tf.cast(correct_prediction, tf.float32))) * 100\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "                             \n",
    "merged = tf.summary.merge_all()\n",
    "                             \n",
    "test_data = mnist.test.images[:batch_size].reshape((-1, time_steps, element_size))\n",
    "test_labels = mnist.test.labels[:batch_size]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    train_writer = tf.summary.FileWriter(LOG_DIR + '/train', graph=tf.get_default_graph())\n",
    "    test_writer = tf.summary.FileWriter(LOG_DIR + '/test', graph=tf.get_default_graph())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "                             \n",
    "    for i in range(10000):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        batch_x = batch_x.reshape((batch_size, time_steps, element_size))\n",
    "        summary, _ = sess.run([merged, train_step], feed_dict={_inputs:batch_x, y:batch_y})\n",
    "        train_writer.add_summary(summary, i)\n",
    "        if i % 1000 == 0:\n",
    "            acc, loss = sess.run([accuracy, cross_entropy], feed_dict={_inputs: batch_x, y: batch_y})\n",
    "                             \n",
    "            print(\n",
    "                'Iter ' + str(i) + ', Minibatch loss= ' + \\\n",
    "                '{:.6f}'.format(loss) + ', Training accuracy= ' + \\\n",
    "                '{:.5f}'.format(acc)\n",
    "            )\n",
    "                             \n",
    "        if i % 10 == 0:\n",
    "            summary, acc = sess.run([merged, accuracy], feed_dict={_inputs: test_data, y: test_labels})\n",
    "            test_writer.add_summary(summary, i)\n",
    "                             \n",
    "    test_acc = sess.run(accuracy, feed_dict={_inputs: test_data, y: test_labels})\n",
    "    print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-611644370306>:4: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From <ipython-input-1-611644370306>:34: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Iter 0, Minibatch loss= 2.305527, Training accuracy= 4.68750\n",
      "Iter 1000, Minibatch loss= 0.193272, Training accuracy= 95.31250\n",
      "Iter 2000, Minibatch loss= 0.254613, Training accuracy= 92.96875\n",
      "Iter 3000, Minibatch loss= 0.035283, Training accuracy= 99.21875\n",
      "Test accuracy: 97.6562\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets('/tmp/data', one_hot=True)\n",
    "\n",
    "element_size = 28\n",
    "time_steps = 28\n",
    "num_classes = 10\n",
    "batch_size = 128\n",
    "hidden_layer_size = 128\n",
    "\n",
    "_inputs = tf.placeholder(tf.float32, shape=[None, time_steps, element_size], name='inputs')\n",
    "y = tf.placeholder(tf.float32, shape=[None, num_classes], name='labels')\n",
    "\n",
    "rnn_cell = tf.contrib.rnn.BasicRNNCell(hidden_layer_size)\n",
    "outputs, _ = tf.nn.dynamic_rnn(rnn_cell, _inputs, dtype=tf.float32)\n",
    "       \n",
    "Wl = tf.Variable(tf.truncated_normal([\n",
    "    hidden_layer_size,\n",
    "    num_classes\n",
    "], mean=0, stddev=.01))\n",
    "\n",
    "bl = tf.Variable(tf.truncated_normal(\n",
    "    [num_classes],\n",
    "    mean=0, stddev=.01\n",
    "))\n",
    "\n",
    "def get_linear_layer(vector):\n",
    "    return tf.matmul(vector, Wl) + bl\n",
    "\n",
    "last_rnn_output = outputs[:,-1,:]\n",
    "final_output = get_linear_layer(last_rnn_output)\n",
    "\n",
    "softmax = tf.nn.softmax_cross_entropy_with_logits(logits=final_output, labels=y)\n",
    "cross_entropy = tf.reduce_mean(softmax)\n",
    "                             \n",
    "train_step = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cross_entropy)\n",
    "    \n",
    "correct_prediction = tf.equal(\n",
    "    tf.argmax(y, 1),\n",
    "    tf.argmax(final_output, 1)\n",
    ")\n",
    "accuracy = (tf.reduce_mean(tf.cast(correct_prediction, tf.float32))) * 100\n",
    "                             \n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "                                 \n",
    "test_data = mnist.test.images[:batch_size].reshape((-1, time_steps, element_size))\n",
    "test_labels = mnist.test.labels[:batch_size]\n",
    "          \n",
    "for i in range(3001):\n",
    "    batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "    batch_x = batch_x.reshape((batch_size, time_steps, element_size))\n",
    "    sess.run(train_step, feed_dict={_inputs:batch_x, y:batch_y})\n",
    "    if i % 1000 == 0:\n",
    "        acc, loss = sess.run([accuracy, cross_entropy], feed_dict={_inputs: batch_x, y: batch_y})\n",
    "        print(\n",
    "            'Iter ' + str(i) + ', Minibatch loss= ' + \\\n",
    "            '{:.6f}'.format(loss) + ', Training accuracy= ' + \\\n",
    "            '{:.5f}'.format(acc)\n",
    "        )\n",
    "\n",
    "test_acc = sess.run(accuracy, feed_dict={_inputs: test_data, y: test_labels})\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['six eight two eight four six', 'two eight six eight pad pad', 'two two six six pad pad', 'eight eight four pad pad pad', 'eight two eight eight pad pad', 'two two eight pad pad pad']\n",
      "['seven one one seven nine three', 'nine five three nine pad pad', 'three nine one three pad pad', 'five one five pad pad pad', 'seven five three five pad pad', 'nine five three pad pad pad']\n",
      "[6, 4, 4, 3, 4, 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "digit_to_word_map = {\n",
    "    1: 'One', 2: 'Two', 3: 'Three', 4: 'Four', 5: 'Five',\n",
    "    6: 'Six', 7: 'Seven', 8: 'Eight', 9: 'Nine'\n",
    "}\n",
    "\n",
    "digit_to_word_map[0] = 'PAD'\n",
    "\n",
    "even_sentences = []\n",
    "odd_sentences = []\n",
    "seqlens = []\n",
    "for i in range(10000):\n",
    "    rand_seq_len = np.random.choice(range(3,7))\n",
    "    seqlens.append(rand_seq_len)\n",
    "    rand_odd_ints = np.random.choice(range(1,10,2), rand_seq_len)\n",
    "    rand_even_ints = np.random.choice(range(2,10,2), rand_seq_len)    \n",
    "    \n",
    "    if rand_seq_len < 6:\n",
    "        rand_odd_ints = np.append(rand_odd_ints, [0]*(6-rand_seq_len))\n",
    "        rand_even_ints = np.append(rand_even_ints, [0]*(6-rand_seq_len))        \n",
    "        \n",
    "    even_sentences.append(' '.join([digit_to_word_map[r] for r in rand_even_ints]))\n",
    "    odd_sentences.append(' '.join([digit_to_word_map[r] for r in rand_odd_ints]))    \n",
    "    \n",
    "data = even_sentences + odd_sentences\n",
    "seqlens *= 2\n",
    "    \n",
    "print(even_sentences[0:6])\n",
    "print(odd_sentences[0:6])\n",
    "print(seqlens[0:6])\n",
    "\n",
    "word2index_map = {}\n",
    "index = 0\n",
    "for sent in data:\n",
    "    for word in sent.lower().split():\n",
    "        if word not in word2index_map:\n",
    "            word2index_map[word] = index\n",
    "            index += 1\n",
    "\n",
    "index2word_map = {index: word for word, index in word2index_map.items()}\n",
    "vocabulary_size = len(index2word_map)\n",
    "\n",
    "labels = [1]*10000 + [0]*10000\n",
    "for i in range(len(labels)):\n",
    "    label = labels[i]\n",
    "    one_hot_encoding = [0]*2\n",
    "    one_hot_encoding[label] = 1\n",
    "    labels[i] = one_hot_encoding\n",
    "    \n",
    "data_indices = list(range(len(data)))\n",
    "np.random.shuffle(data_indices)\n",
    "data = np.array(data)[data_indices]\n",
    "\n",
    "labels = np.array(labels)[data_indices]\n",
    "seqlens = np.array(seqlens)[data_indices]\n",
    "\n",
    "train_x = data[:10000]\n",
    "train_y = labels[:10000]\n",
    "train_seqlens = seqlens[:10000]\n",
    "\n",
    "test_x = data[10000:]\n",
    "test_y = labels[10000:]\n",
    "test_seqlens = seqlens[10000:]\n",
    "        \n",
    "def get_sentence_batch(batch_size, data_x, data_y, data_seqlens):\n",
    "    instance_indices = list(range(len(data_x)))\n",
    "    np.random.shuffle(instance_indices)\n",
    "    batch = instance_indices[:batch_size]\n",
    "    x = [[word2index_map[word] for word in data_x[i].lower().split()] for i in batch]\n",
    "    y = [data_y[i] for i in batch]\n",
    "    seqlens = [data_seqlens[i] for i in batch]\n",
    "    return x, y, seqlens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Six Eight Eight Four PAD PAD', 'Six Two Four Two PAD PAD', 'Six Four Eight Six PAD PAD', 'Four Four Two Four Six Eight', 'Two Two Two PAD PAD PAD', 'Two Six Two Six PAD PAD']\n",
      "['One Nine Nine Three PAD PAD', 'Five Nine Nine Seven PAD PAD', 'Seven Nine One Nine PAD PAD', 'Three Five Seven One Five Seven', 'One Nine Three PAD PAD PAD', 'Seven Three Five Three PAD PAD']\n",
      "[4, 4, 4, 6, 3, 4]\n",
      "WARNING:tensorflow:From <ipython-input-1-ee4ae062d50d>:110: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Accuracy at 0: 77.34375\n",
      "Accuracy at 100: 91.40625\n",
      "Accuracy at 200: 100.00000\n",
      "Accuracy at 300: 100.00000\n",
      "Accuracy at 400: 100.00000\n",
      "Accuracy at 500: 100.00000\n",
      "Accuracy at 600: 100.00000\n",
      "Accuracy at 700: 100.00000\n",
      "Accuracy at 800: 100.00000\n",
      "Accuracy at 900: 100.00000\n",
      "Test batch accuracy 0: 100.00000\n",
      "Test batch accuracy 1: 100.00000\n",
      "Test batch accuracy 2: 100.00000\n",
      "Test batch accuracy 3: 100.00000\n",
      "Test batch accuracy 4: 100.00000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 128\n",
    "embedding_dimension = 64\n",
    "num_classes = 2\n",
    "hidden_layer_size = 32\n",
    "time_steps = 6\n",
    "element_size = 1\n",
    "\n",
    "_inputs = tf.placeholder(tf.int32, shape=[batch_size, time_steps])\n",
    "_labels = tf.placeholder(tf.float32, shape=[batch_size, num_classes])\n",
    "_seqlens = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "\n",
    "with tf.name_scope('embeddings'):\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size, embedding_dimension], -1.0, 1.0, name='embedding')\n",
    "    )\n",
    "    embed = tf.nn.embedding_lookup(embeddings, _inputs)\n",
    "    \n",
    "num_lstm_layers = 2    \n",
    "with tf.variable_scope('lstm'):\n",
    "    cells = [tf.nn.rnn_cell.LSTMCell(num_units=hidden_layer_size, forget_bias=1.0) for n in range(num_lstm_layers)]\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell(cells=cells, state_is_tuple=True)\n",
    "    outputs, states = tf.nn.dynamic_rnn(cell, embed, sequence_length=_seqlens, dtype=tf.float32)\n",
    "    weights = {\n",
    "        'linear_layer': tf.Variable(tf.truncated_normal([hidden_layer_size, num_classes], mean=0, stddev=.01))\n",
    "    }\n",
    "    biases = {\n",
    "        'linear_layer': tf.Variable(tf.truncated_normal([num_classes], mean=0, stddev=.01))\n",
    "    }    \n",
    "    \n",
    "    final_output = tf.matmul(\n",
    "        states[num_lstm_layers-1][1], \n",
    "        weights['linear_layer'] + biases['linear_layer']\n",
    "    )\n",
    "    \n",
    "    softmax = tf.nn.softmax_cross_entropy_with_logits(logits=final_output, labels=_labels)\n",
    "    cross_entropy = tf.reduce_mean(softmax)\n",
    "    \n",
    "train_step = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(_labels, 1), tf.argmax(final_output, 1))\n",
    "accuracy = (tf.reduce_mean(tf.cast(correct_prediction, tf.float32))) * 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(1000):\n",
    "        x_batch, y_batch, seqlen_batch = get_sentence_batch(\n",
    "            batch_size, train_x, train_y, train_seqlens\n",
    "        )\n",
    "        \n",
    "        sess.run(train_step, feed_dict={\n",
    "            _inputs: x_batch, _labels: y_batch,\n",
    "            _seqlens: seqlen_batch\n",
    "        })\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            acc = sess.run(accuracy, feed_dict={\n",
    "                _inputs: x_batch, _labels: y_batch,\n",
    "                _seqlens: seqlen_batch\n",
    "            })\n",
    "            print('Accuracy at %d: %.5f' % (step, acc))\n",
    "            \n",
    "    for test_batch in range(5):\n",
    "        x_test, y_test, seqlen_test = get_sentence_batch(\n",
    "            batch_size, test_x, test_y, test_seqlens\n",
    "        )\n",
    "        batch_pred, batch_acc = sess.run([tf.argmax(final_output, 1), accuracy], feed_dict={\n",
    "            _inputs: x_test, _labels: y_test, _seqlens: seqlen_test\n",
    "        })\n",
    "        print('Test batch accuracy %d: %.5f' % (test_batch, batch_acc))\n",
    "        \n",
    "    output_example = sess.run([states[1]], feed_dict={\n",
    "        _inputs: x_test, _labels: y_test, _seqlens: seqlen_test\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.85159481  0.8767556   0.81847155  0.53614092  0.77752185 -0.77815473\n",
      "   0.8657003   0.90827566  0.85671753  0.85030675  0.8425433   0.12184782\n",
      "  -0.14983748  0.79968351  0.74662387  0.93928504  0.87972701  0.85356337\n",
      "   0.93362337 -0.23900993 -0.81739652  0.82529616  0.29782882  0.86127776\n",
      "   0.9007405   0.92068565  0.86978394  0.95799339  0.87370455  0.88025647\n",
      "   0.44403982 -0.91754037]\n",
      " [-0.8927322  -0.68893147 -0.83845693 -0.90860337  0.34355116  0.08332121\n",
      "  -0.8036803  -0.29576114 -0.85738522 -0.85630363 -0.90647244  0.68648583\n",
      "  -0.80262917 -0.9019931  -0.92109734 -0.39426142 -0.74153823 -0.64433724\n",
      "  -0.71697038  0.84186882 -0.07649516 -0.86302274 -0.80284071 -0.92036057\n",
      "  -0.8720457  -0.80489385 -0.83608991 -0.37448558 -0.30559519 -0.92124712\n",
      "  -0.86384159  0.00238653]\n",
      " [-0.87023741 -0.64196301 -0.79995406 -0.88233733  0.34151122  0.0774629\n",
      "  -0.75840443 -0.28438944 -0.82539082 -0.81892163 -0.86033899  0.63037717\n",
      "  -0.7231133  -0.86606294 -0.89985114 -0.35535118 -0.67129922 -0.6258688\n",
      "  -0.67172903  0.78827602 -0.03316363 -0.81351018 -0.730048   -0.88715535\n",
      "  -0.82495195 -0.74434835 -0.80757427 -0.35635796 -0.31005391 -0.88927794\n",
      "  -0.82380021  0.01024541]]\n"
     ]
    }
   ],
   "source": [
    "print(output_example[0][1][0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logs/word2vec_intro/metadata.tsv\n",
      "Loss at 0: 7.36153\n",
      "Loss at 100: 3.02967\n",
      "Loss at 200: 2.83003\n",
      "Loss at 300: 2.70158\n",
      "Loss at 400: 2.59275\n",
      "Loss at 500: 2.54657\n",
      "Loss at 600: 2.55289\n",
      "Loss at 700: 2.54934\n",
      "Loss at 800: 2.54687\n",
      "Loss at 900: 2.50954\n",
      "nine\n",
      "0.847617\n",
      "seven\n",
      "0.844186\n",
      "five\n",
      "0.742424\n",
      "three\n",
      "0.596183\n",
      "six\n",
      "0.080226\n",
      "four\n",
      "0.039013\n",
      "eight\n",
      "-0.130936\n",
      "two\n",
      "-0.309091\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "batch_size = 64\n",
    "embedding_dimension = 5\n",
    "negative_samples = 8\n",
    "LOG_DIR = 'logs/word2vec_intro'\n",
    "\n",
    "digit_to_word_map = {\n",
    "    1: 'One', 2: 'Two', 3: 'Three', 4: 'Four', 5: 'Five',\n",
    "    6: 'Six', 7: 'Seven', 8: 'Eight', 9: 'Nine'\n",
    "}\n",
    "\n",
    "sentences = []\n",
    "for i in range(10000):\n",
    "    rand_odd_ints = np.random.choice(range(1, 10, 2), 3)\n",
    "    sentences.append(' '.join([digit_to_word_map[r] for r in rand_odd_ints]))\n",
    "    rand_even_ints = np.random.choice(range(2, 10, 2), 3)    \n",
    "    sentences.append(' '.join([digit_to_word_map[r] for r in rand_even_ints]))    \n",
    "\n",
    "word2index_map = {}\n",
    "index = 0\n",
    "for sent in sentences:\n",
    "    for word in sent.lower().split():\n",
    "        if word not in word2index_map:\n",
    "            word2index_map[word] = index\n",
    "            index += 1\n",
    "\n",
    "index2word_map = {index: word for word, index in word2index_map.items()}\n",
    "vocabulary_size = len(index2word_map)\n",
    "\n",
    "skip_gram_pairs = []\n",
    "for sent in sentences:\n",
    "    tokenized_sent = sent.lower().split()\n",
    "    for i in range(1, len(tokenized_sent)-1):\n",
    "        word_context_pair = [\n",
    "            [word2index_map[tokenized_sent[i-1]], word2index_map[tokenized_sent[i+1]]],\n",
    "            word2index_map[tokenized_sent[i]]\n",
    "        ]\n",
    "        \n",
    "        skip_gram_pairs.append([word_context_pair[1], word_context_pair[0][0]])\n",
    "        skip_gram_pairs.append([word_context_pair[1], word_context_pair[0][1]])        \n",
    "        \n",
    "def get_skipgram_batch(batch_size):\n",
    "    instance_indices = list(range(len(skip_gram_pairs)))\n",
    "    np.random.shuffle(instance_indices)\n",
    "    batch = instance_indices[:batch_size]\n",
    "    x = [skip_gram_pairs[i][0] for i in batch]\n",
    "    y = [[skip_gram_pairs[i][1]] for i in batch]    \n",
    "    return x, y\n",
    "\n",
    "train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "\n",
    "with tf.name_scope('embeddings'):\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size, embedding_dimension], -1.0, 1.0, name='embedding')\n",
    "    )\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "nce_weights = tf.Variable(tf.truncated_normal(\n",
    "    [vocabulary_size, embedding_dimension], \n",
    "    stddev=1.0 / math.sqrt(embedding_dimension)\n",
    "))\n",
    "\n",
    "nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "loss = tf.reduce_mean(tf.nn.nce_loss(\n",
    "    weights=nce_weights, biases=nce_biases, inputs=embed,\n",
    "    labels=train_labels, num_sampled=negative_samples, num_classes=vocabulary_size\n",
    "))\n",
    "tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "    learning_rate=0.1,\n",
    "    global_step=global_step,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.95,\n",
    "    staircase=True\n",
    ")\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "with tf.Session() as sess:\n",
    "    train_writer = tf.summary.FileWriter(LOG_DIR, graph=tf.get_default_graph())\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    with open(os.path.join(LOG_DIR, 'metadata.tsv'), 'w') as metadata:\n",
    "        metadata.write('Name\\tClass\\n')\n",
    "        for k, v in index2word_map.items():\n",
    "            metadata.write('%s\\t%s\\n' % (v, k))\n",
    "            \n",
    "    config = projector.ProjectorConfig()\n",
    "    embedding = config.embeddings.add()\n",
    "    embedding.tensor_name = embeddings.name\n",
    "    embedding.metadata_path = os.path.join(LOG_DIR, 'metadata.tsv')\n",
    "    print(embedding.metadata_path)\n",
    "    projector.visualize_embeddings(train_writer, config)\n",
    "    \n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    for step in range(1000):\n",
    "        x_batch, y_batch = get_skipgram_batch(batch_size)\n",
    "        summary, _ = sess.run(\n",
    "            [merged, train_step], \n",
    "            feed_dict={train_inputs: x_batch, train_labels: y_batch}\n",
    "        )\n",
    "        train_writer.add_summary(summary, step)\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            saver.save(sess, os.path.join(LOG_DIR, 'w2v_model.ckpt'), step)\n",
    "            loss_value = sess.run(loss, feed_dict={\n",
    "                train_inputs: x_batch,\n",
    "                train_labels: y_batch\n",
    "            })        \n",
    "            print('Loss at %d: %.5f' % (step, loss_value))\n",
    "        \n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    normalized_embeddings_matrix = sess.run(normalized_embeddings)\n",
    "    \n",
    "ref_word = normalized_embeddings_matrix[word2index_map['one']]\n",
    "cosine_dists = np.dot(normalized_embeddings_matrix, ref_word)\n",
    "ff = np.argsort(cosine_dists)[::-1][1:10]\n",
    "for f in ff:\n",
    "    print(index2word_map[f])\n",
    "    print(cosine_dists[f])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['two six six two pad pad', 'eight eight six two eight six', 'six eight two two two six', 'six six eight four pad pad', 'eight six four pad pad pad', 'eight two eight eight pad pad']\n",
      "['three one five seven pad pad', 'one three three five five nine', 'nine one one one five seven', 'three three five five pad pad', 'one five three pad pad pad', 'one nine seven seven pad pad']\n",
      "[4, 6, 6, 4, 3, 4]\n",
      "one\n",
      "two\n",
      "three\n",
      "four\n",
      "five\n",
      "six\n",
      "seven\n",
      "eight\n",
      "nine\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "digit_to_word_map = {\n",
    "    1: 'one', 2: 'two', 3: 'three', 4: 'four', 5: 'five',\n",
    "    6: 'six', 7: 'seven', 8: 'eight', 9: 'nine'\n",
    "}\n",
    "\n",
    "digit_to_word_map[0] = 'pad'\n",
    "\n",
    "even_sentences = []\n",
    "odd_sentences = []\n",
    "seqlens = []\n",
    "for i in range(10000):\n",
    "    rand_seq_len = np.random.choice(range(3,7))\n",
    "    seqlens.append(rand_seq_len)\n",
    "    rand_odd_ints = np.random.choice(range(1,10,2), rand_seq_len)\n",
    "    rand_even_ints = np.random.choice(range(2,10,2), rand_seq_len)    \n",
    "    \n",
    "    if rand_seq_len < 6:\n",
    "        rand_odd_ints = np.append(rand_odd_ints, [0]*(6-rand_seq_len))\n",
    "        rand_even_ints = np.append(rand_even_ints, [0]*(6-rand_seq_len))        \n",
    "        \n",
    "    even_sentences.append(' '.join([digit_to_word_map[r] for r in rand_even_ints]))\n",
    "    odd_sentences.append(' '.join([digit_to_word_map[r] for r in rand_odd_ints]))    \n",
    "    \n",
    "data = even_sentences + odd_sentences\n",
    "seqlens *= 2\n",
    "    \n",
    "print(even_sentences[0:6])\n",
    "print(odd_sentences[0:6])\n",
    "print(seqlens[0:6])\n",
    "\n",
    "word2index_map = {}\n",
    "index = 0\n",
    "for sent in data:\n",
    "    for word in sent.lower().split():\n",
    "        if word not in word2index_map:\n",
    "            word2index_map[word] = index\n",
    "            index += 1\n",
    "\n",
    "index2word_map = {index: word for word, index in word2index_map.items()}\n",
    "vocabulary_size = len(index2word_map)\n",
    "\n",
    "labels = [1]*10000 + [0]*10000\n",
    "for i in range(len(labels)):\n",
    "    label = labels[i]\n",
    "    one_hot_encoding = [0]*2\n",
    "    one_hot_encoding[label] = 1\n",
    "    labels[i] = one_hot_encoding\n",
    "    \n",
    "data_indices = list(range(len(data)))\n",
    "np.random.shuffle(data_indices)\n",
    "data = np.array(data)[data_indices]\n",
    "\n",
    "labels = np.array(labels)[data_indices]\n",
    "seqlens = np.array(seqlens)[data_indices]\n",
    "\n",
    "train_x = data[:10000]\n",
    "train_y = labels[:10000]\n",
    "train_seqlens = seqlens[:10000]\n",
    "\n",
    "test_x = data[10000:]\n",
    "test_y = labels[10000:]\n",
    "test_seqlens = seqlens[10000:]\n",
    "\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "path_to_glove = 'data/glove.840B.300d.zip'\n",
    "PRE_TRAINED = True\n",
    "GLOVE_SIZE = 300\n",
    "batch_size = 128\n",
    "embedding_dimension = 64\n",
    "num_classes = 2\n",
    "hidden_layer_size = 32\n",
    "time_steps = 6\n",
    "\n",
    "def get_glove(path_to_glove, word2index_map):\n",
    "    embedding_weights = {}\n",
    "    count_all_words = 0\n",
    "    \n",
    "    with zipfile.ZipFile(path_to_glove) as z:\n",
    "        with z.open('glove.840B.300d.txt') as f:\n",
    "            for line in f:\n",
    "                vals = line.split()\n",
    "                word = str(vals[0].decode(\"utf-8\"))\n",
    "                \n",
    "                if word in word2index_map:\n",
    "                    print(word)\n",
    "                    count_all_words += 1\n",
    "                    coefs = np.asarray(vals[1:], dtype='float32')\n",
    "                    coefs /= np.linalg.norm(coefs)\n",
    "                    embedding_weights[word] = coefs\n",
    "                    \n",
    "                if count_all_words == vocabulary_size - 1:\n",
    "                    break   \n",
    "                    \n",
    "    return embedding_weights\n",
    "\n",
    "word2embedding_dict = get_glove(path_to_glove, word2index_map)\n",
    "embedding_matrix = np.zeros((vocabulary_size, GLOVE_SIZE))\n",
    "for word, index in word2index_map.items():\n",
    "    if not word == \"pad\":\n",
    "        word_embedding = word2embedding_dict[word]\n",
    "        embedding_matrix[index,:] = word_embedding\n",
    "        \n",
    "data_indices = list(range(len(data)))\n",
    "np.random.shuffle(data_indices)\n",
    "data = np.array(data)[data_indices]\n",
    "labels = np.array(labels)[data_indices]\n",
    "seqlens = np.array(seqlens)[data_indices]\n",
    "train_x = data[:10000]\n",
    "train_y = labels[:10000]\n",
    "train_seqlens = seqlens[:10000]\n",
    "test_x = data[10000:]\n",
    "test_y = labels[10000:]\n",
    "test_seqlens = seqlens[10000:]\n",
    "\n",
    "def get_sentence_batch(batch_size, data_x, data_y, data_seqlens):\n",
    "    instance_indices = list(range(len(data_x)))\n",
    "    np.random.shuffle(instance_indices)\n",
    "    batch = instance_indices[:batch_size]\n",
    "    x = [[word2index_map[word] for word in data_x[i].split()] for i in batch]\n",
    "    y = [data_y[i] for i in batch]\n",
    "    seqlens = [data_seqlens[i] for i in batch]\n",
    "    return x, y, seqlens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-c21df33ccc4e>:45: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Accuracy at 0: 53.12500\n",
      "Accuracy at 100: 100.00000\n",
      "Accuracy at 200: 100.00000\n",
      "Accuracy at 300: 100.00000\n",
      "Accuracy at 400: 100.00000\n",
      "Accuracy at 500: 100.00000\n",
      "Accuracy at 600: 100.00000\n",
      "Accuracy at 700: 100.00000\n",
      "Accuracy at 800: 100.00000\n",
      "Accuracy at 900: 100.00000\n",
      "Test batch accuracy 0: 100.00000\n",
      "Test batch accuracy 1: 100.00000\n",
      "Test batch accuracy 2: 100.00000\n",
      "Test batch accuracy 3: 100.00000\n",
      "Test batch accuracy 4: 100.00000\n"
     ]
    }
   ],
   "source": [
    "_inputs = tf.placeholder(tf.int32, shape=[batch_size, time_steps])\n",
    "embedding_placeholder = tf.placeholder(tf.float32, [vocabulary_size, GLOVE_SIZE])\n",
    "_labels = tf.placeholder(tf.float32, shape=[batch_size, num_classes])\n",
    "_seqlens = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "\n",
    "if PRE_TRAINED:\n",
    "    embeddings = tf.Variable(tf.constant(0.0, shape=[vocabulary_size, GLOVE_SIZE]), trainable=True)\n",
    "    embedding_init = embeddings.assign(embedding_placeholder)\n",
    "    embed = tf.nn.embedding_lookup(embeddings, _inputs)\n",
    "else:\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_dimension], -1.0, 1.0))\n",
    "    embed = tf.nn.embedding_lookup(embeddings, _inputs)\n",
    "    \n",
    "x, y, seqlens = get_sentence_batch(10, train_x, train_y, train_seqlens)\n",
    "\n",
    "with tf.name_scope('biGRU'):\n",
    "    with tf.variable_scope('forward'):\n",
    "        gru_fw_cell = tf.contrib.rnn.GRUCell(hidden_layer_size)\n",
    "        gru_fw_cell = tf.contrib.rnn.DropoutWrapper(gru_fw_cell)\n",
    "        \n",
    "    with tf.variable_scope('backward'):\n",
    "        gru_bw_cell = tf.contrib.rnn.GRUCell(hidden_layer_size)\n",
    "        gru_bw_cell = tf.contrib.rnn.DropoutWrapper(gru_bw_cell)\n",
    "\n",
    "    outputs, states = tf.nn.bidirectional_dynamic_rnn(\n",
    "        cell_fw=gru_fw_cell,\n",
    "        cell_bw=gru_bw_cell,\n",
    "        inputs=embed,\n",
    "        sequence_length=_seqlens,\n",
    "        dtype=tf.float32,\n",
    "        scope='BiGRU'\n",
    "    )\n",
    "    \n",
    "    states = tf.concat(values=states, axis=1)\n",
    "    \n",
    "weights = {\n",
    "    'linear_layer': tf.Variable(tf.truncated_normal([2*hidden_layer_size, num_classes], mean=0, stddev=.01))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'linear_layer': tf.Variable(tf.truncated_normal([num_classes], mean=0, stddev=.01))\n",
    "}\n",
    "\n",
    "final_output = tf.matmul(states, weights['linear_layer']) + biases['linear_layer']\n",
    "softmax = tf.nn.softmax_cross_entropy_with_logits(logits=final_output, labels=_labels)\n",
    "cross_entropy = tf.reduce_mean(softmax)\n",
    "\n",
    "train_step = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(_labels, 1), tf.argmax(final_output, 1))\n",
    "accuracy = (tf.reduce_mean(tf.cast(correct_prediction, tf.float32))) * 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(embedding_init, feed_dict={embedding_placeholder: embedding_matrix})\n",
    "    for step in range(1000):\n",
    "        x_batch, y_batch, seqlen_batch = get_sentence_batch(\n",
    "            batch_size, train_x, train_y, train_seqlens\n",
    "        )\n",
    "        sess.run(train_step, feed_dict={\n",
    "            _inputs: x_batch, _labels: y_batch, _seqlens: seqlen_batch\n",
    "        })\n",
    "        if step % 100 == 0:\n",
    "            acc = sess.run(accuracy, feed_dict={\n",
    "                _inputs: x_batch, _labels: y_batch, _seqlens: seqlen_batch\n",
    "            })\n",
    "            print('Accuracy at %d: %.5f' % (step, acc))\n",
    "            \n",
    "    for test_batch in range(5):\n",
    "        x_test, y_test, seqlen_test = get_sentence_batch(\n",
    "            batch_size, test_x, test_y, test_seqlens\n",
    "        )\n",
    "        batch_pred, batch_acc = sess.run([tf.argmax(final_output, 1), accuracy], feed_dict={\n",
    "            _inputs: x_test, _labels: y_test, _seqlens: seqlen_test\n",
    "        })\n",
    "        print('Test batch accuracy %d: %.5f' % (test_batch, batch_acc))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
