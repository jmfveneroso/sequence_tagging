{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hello World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'HelloWorld'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "h = tf.constant('Hello')\n",
    "w = tf.constant('World')\n",
    "hw = h + w\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    ans = sess.run(hw)    \n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Regression on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "DATA_DIR = '/tmp/data'\n",
    "NUM_STEPS = 1000\n",
    "MINIBATCH_SIZE = 100\n",
    "\n",
    "data = input_data.read_data_sets(DATA_DIR, one_hot=True)\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "\n",
    "y_true = tf.placeholder(tf.float32, [None, 10])\n",
    "y_pred = tf.matmul(x, W)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred, labels=y_true))\n",
    "\n",
    "gd_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "correct_mask = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y_true, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_mask, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for _ in range(NUM_STEPS):\n",
    "        batch_xs, batch_ys = data.train.next_batch(MINIBATCH_SIZE)\n",
    "        sess.run(gd_step, feed_dict={x: batch_xs, y_true: batch_ys})\n",
    "        \n",
    "    ans = sess.run(accuracy, feed_dict={x: data.test.images, y_true: data.test.labels})    \n",
    "    \n",
    "print(\"Accuracy: {:.4}%\".format(ans*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(5)\n",
    "b = tf.constant(2)\n",
    "c = tf.constant(3)\n",
    "d = tf.multiply(a, b)\n",
    "e = tf.add(c, b)\n",
    "f = tf.subtract(d, e)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    fetches = [a, b, c, d, e, f]\n",
    "    outs = sess.run(fetches)\n",
    "\n",
    "print(\"outs: {}\".format(outs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(1)\n",
    "b = tf.constant(2)\n",
    "c = tf.multiply(a, b)\n",
    "d = tf.add(a, b)\n",
    "e = tf.subtract(c, d)\n",
    "f = tf.add(c, d)\n",
    "g = tf.divide(e, f)\n",
    "\n",
    "sess = tf.Session()\n",
    "outs = sess.run(g)\n",
    "\n",
    "print('Outs: {}'.format(outs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(1.)\n",
    "b = tf.constant(2.)\n",
    "c = tf.multiply(a, b)\n",
    "d = tf.sin(c)\n",
    "e = tf.divide(d, b)\n",
    "\n",
    "sess = tf.Session()\n",
    "outs = sess.run(e)\n",
    "print('Outs: {}'.format(outs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "g1 = tf.get_default_graph()\n",
    "g2 = tf.Graph()\n",
    "\n",
    "with g2.as_default():\n",
    "    print(g1 is tf.get_default_graph())\n",
    "a = tf.constant(5)\n",
    "\n",
    "print(a.graph is g)\n",
    "print(a.graph is tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant([1, 2, 3], name='x', dtype=tf.float64)\n",
    "print(x)\n",
    "x = tf.cast(x, tf.int64, name='cast_x')\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "c = tf.constant([[1,2,3],[4,5,6]])\n",
    "print('c: {}'.format(c.get_shape()))\n",
    "\n",
    "c = tf.constant(np.array([\n",
    "    [[1,2,3],[4,5,6]],\n",
    "    [[1,1,1],[2,2,2]]\n",
    "]))\n",
    "print('c: {}'.format(c.get_shape()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "c = tf.linspace(0.0, 4.0, 5)\n",
    "print('c: {}'.format(c.eval()))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "(3, 1)\n",
      "matmul result: [[ 4]\n",
      " [10]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "A = tf.constant([[1,2,3],[4,5,6]])\n",
    "print(A.get_shape())\n",
    "\n",
    "x = tf.constant([1,0,1])\n",
    "x = tf.expand_dims(x, 1)\n",
    "print(x.get_shape())\n",
    "\n",
    "b = tf.matmul(A, x)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "print('matmul result: {}'.format(b.eval()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre run: <tf.Variable 'var_5:0' shape=(1, 5) dtype=float32_ref>\n",
      "post run: [[ 0.07406364 -0.37498748  0.24438557  1.5451785  -0.07271151]]\n"
     ]
    }
   ],
   "source": [
    "init_val = tf.random_normal((1,5),0,1)\n",
    "var = tf.Variable(init_val, name='var')\n",
    "print('pre run: {}'.format(var))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    post_var = sess.run(var)\n",
    "    \n",
    "print('post run: {}'.format(post_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outs = 3.251608371734619\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_data = np.random.randn(5,10)\n",
    "w_data = np.random.randn(10,1)\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    x = tf.placeholder(tf.float32,shape=(5,10))\n",
    "    w = tf.placeholder(tf.float32,shape=(10,1))\n",
    "    b = tf.fill((5,1),-1.)\n",
    "    xw = tf.matmul(x, w)\n",
    "    xwb = xw + b\n",
    "    s = tf.reduce_max(xwb)\n",
    "    with tf.Session() as sess:\n",
    "        outs = sess.run(s, feed_dict={x: x_data, w: w_data})\n",
    "        \n",
    "print('outs = {}'.format(outs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [array([[0.29944515, 0.52722275, 0.10362744]], dtype=float32), -0.20495152]\n",
      "5 [array([[0.29500726, 0.5001984 , 0.10093738]], dtype=float32), -0.20034765]\n",
      "10 [array([[0.29500726, 0.5001984 , 0.10093737]], dtype=float32), -0.20034763]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate data.\n",
    "x_data = np.random.randn(2000,3)\n",
    "w_real = [0.3,0.5,0.1]\n",
    "b_real = -0.2\n",
    "\n",
    "noise = np.random.randn(1,2000)*0.1\n",
    "y_data = np.matmul(w_real, x_data.T) + b_real + noise\n",
    "\n",
    "\n",
    "# Model.\n",
    "NUM_STEPS = 10\n",
    "\n",
    "g = tf.Graph()\n",
    "wb_ = []\n",
    "with g.as_default():\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "    y_true = tf.placeholder(tf.float32, shape=None)\n",
    "    \n",
    "    with tf.name_scope('inference') as scope:\n",
    "        w = tf.Variable([[0,0,0]],dtype=tf.float32,name='weights')\n",
    "        b = tf.Variable(0,dtype=tf.float32,name='bias')\n",
    "        y_pred = tf.matmul(w, tf.transpose(x)) + b\n",
    "\n",
    "    with tf.name_scope('loss') as scope:\n",
    "        loss = tf.reduce_mean(tf.square(y_true-y_pred))\n",
    "\n",
    "    with tf.name_scope('train') as scope:\n",
    "        learning_rate = 0.5\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        train = optimizer.minimize(loss)\n",
    "        \n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for step in range(NUM_STEPS):\n",
    "            sess.run(train,{x:x_data,y_true:y_data})\n",
    "            if (step % 5 == 0):\n",
    "                print(step, sess.run([w,b]))\n",
    "                wb_.append(sess.run([w,b]))\n",
    "        \n",
    "        print(10, sess.run([w,b]))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [array([[0.00928201, 0.01450536, 0.00252127]], dtype=float32), -0.02111341]\n",
      "5 [array([[0.05440196, 0.08505421, 0.01477653]], dtype=float32), -0.1240038]\n",
      "10 [array([[0.09671815, 0.15131567, 0.02625452]], dtype=float32), -0.2215482]\n",
      "15 [array([[0.13573062, 0.21252505, 0.03680407]], dtype=float32), -0.31304452]\n",
      "20 [array([[0.17134641, 0.26853004, 0.04639117]], dtype=float32), -0.39832264]\n",
      "25 [array([[0.20373096, 0.31957093, 0.05505797]], dtype=float32), -0.47757095]\n",
      "30 [array([[0.23317596, 0.36608312, 0.06288499]], dtype=float32), -0.5511742]\n",
      "35 [array([[0.26001254, 0.4085659 , 0.06996584]], dtype=float32), -0.6195994]\n",
      "40 [array([[0.28456405, 0.4475101 , 0.07639296]], dtype=float32), -0.6833277]\n",
      "45 [array([[0.30712482, 0.48336455, 0.08225114]], dtype=float32), -0.7428184]\n",
      "10 [array([[0.323915  , 0.51009125, 0.08657864]], dtype=float32), -0.7876448]\n"
     ]
    }
   ],
   "source": [
    "N = 20000\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "x_data = np.random.randn(N,3)\n",
    "w_real = [0.3,0.5,0.1]\n",
    "b_real = -0.2\n",
    "\n",
    "wxb = np.matmul(w_real,x_data.T) + b_real\n",
    "\n",
    "y_data_pre_noise = sigmoid(wxb)\n",
    "y_data = np.random.binomial(1,y_data_pre_noise)\n",
    "\n",
    "\n",
    "# Model.\n",
    "NUM_STEPS = 50\n",
    "\n",
    "g = tf.Graph()\n",
    "wb_ = []\n",
    "with g.as_default():\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "    y_true = tf.placeholder(tf.float32, shape=None)\n",
    "    \n",
    "    with tf.name_scope('inference') as scope:\n",
    "        w = tf.Variable([[0,0,0]],dtype=tf.float32,name='weights')\n",
    "        b = tf.Variable(0,dtype=tf.float32,name='bias')\n",
    "        y_pred = tf.matmul(w, tf.transpose(x)) + b\n",
    "        y_pred = tf.sigmoid(y_pred)\n",
    "        \n",
    "    with tf.name_scope('loss') as scope:\n",
    "        loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true,logits=y_pred)\n",
    "        loss = tf.reduce_mean(loss)        \n",
    "        \n",
    "    with tf.name_scope('train') as scope:\n",
    "        learning_rate = 0.5\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        train = optimizer.minimize(loss)\n",
    "        \n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for step in range(NUM_STEPS):\n",
    "            sess.run(train,{x:x_data,y_true:y_data})\n",
    "            if (step % 5 == 0):\n",
    "                print(step, sess.run([w,b]))\n",
    "                wb_.append(sess.run([w,b]))\n",
    "        \n",
    "        print(10, sess.run([w,b]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "    \n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1,1,1,1],padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "def conv_layer(input, shape):\n",
    "    W = weight_variable(shape)\n",
    "    b = bias_variable([shape[3]])\n",
    "    return tf.nn.relu(conv2d(input, W) + b)\n",
    "\n",
    "def full_layer(input, size):\n",
    "    in_size = int(input.get_shape()[1])\n",
    "    W = weight_variable([in_size, size])\n",
    "    b = bias_variable([size])\n",
    "    return tf.matmul(input, W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "Step 0 accuracy 0.05999999865889549\n",
      "Step 100 accuracy 0.9200000166893005\n",
      "Step 200 accuracy 0.9200000166893005\n",
      "Step 300 accuracy 0.9200000166893005\n",
      "Step 400 accuracy 0.9599999785423279\n",
      "Step 500 accuracy 0.9599999785423279\n",
      "Step 600 accuracy 0.9800000190734863\n",
      "Step 700 accuracy 0.9399999976158142\n",
      "Step 800 accuracy 1.0\n",
      "Step 900 accuracy 0.9800000190734863\n",
      "Step 1000 accuracy 0.9599999785423279\n",
      "Step 1100 accuracy 0.9800000190734863\n",
      "Step 1200 accuracy 0.9599999785423279\n",
      "Step 1300 accuracy 0.8999999761581421\n",
      "Step 1400 accuracy 0.9599999785423279\n",
      "Step 1500 accuracy 0.9800000190734863\n",
      "Step 1600 accuracy 1.0\n",
      "Step 1700 accuracy 1.0\n",
      "Step 1800 accuracy 0.9599999785423279\n",
      "Step 1900 accuracy 1.0\n",
      "Step 2000 accuracy 0.9800000190734863\n",
      "Step 2100 accuracy 1.0\n",
      "Step 2200 accuracy 1.0\n",
      "Step 2300 accuracy 1.0\n",
      "Step 2400 accuracy 1.0\n",
      "Step 2500 accuracy 1.0\n",
      "Step 2600 accuracy 1.0\n",
      "Step 2700 accuracy 1.0\n",
      "Step 2800 accuracy 1.0\n",
      "Step 2900 accuracy 0.9800000190734863\n",
      "Step 3000 accuracy 0.9599999785423279\n",
      "Step 3100 accuracy 0.9599999785423279\n",
      "Step 3200 accuracy 0.9800000190734863\n",
      "Step 3300 accuracy 1.0\n",
      "Step 3400 accuracy 1.0\n",
      "Step 3500 accuracy 1.0\n",
      "Step 3600 accuracy 0.9800000190734863\n",
      "Step 3700 accuracy 0.9800000190734863\n",
      "Step 3800 accuracy 1.0\n",
      "Step 3900 accuracy 0.9800000190734863\n",
      "Step 4000 accuracy 1.0\n",
      "Step 4100 accuracy 0.9800000190734863\n",
      "Step 4200 accuracy 0.9800000190734863\n",
      "Step 4300 accuracy 0.9599999785423279\n",
      "Step 4400 accuracy 0.9800000190734863\n",
      "Step 4500 accuracy 1.0\n",
      "Step 4600 accuracy 1.0\n",
      "Step 4700 accuracy 1.0\n",
      "Step 4800 accuracy 0.9800000190734863\n",
      "Step 4900 accuracy 0.9800000190734863\n",
      "Test accuracy: 0.9902000427246094\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "DATA_DIR = '/tmp/data'\n",
    "STEPS = 5000\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "conv1 = conv_layer(x_image, shape=[5, 5, 1, 32])\n",
    "conv1_pool = max_pool_2x2(conv1)\n",
    "\n",
    "conv2 = conv_layer(conv1_pool, shape=[5, 5, 32, 64])\n",
    "conv2_pool = max_pool_2x2(conv2)\n",
    "\n",
    "conv2_flat = tf.reshape(conv2_pool, [-1, 7*7*64])\n",
    "full_1 = tf.nn.relu(full_layer(conv2_flat, 1024))\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "full1_drop = tf.nn.dropout(full_1, keep_prob=keep_prob)\n",
    "\n",
    "y_conv = full_layer(full1_drop, 10)\n",
    "\n",
    "mnist = input_data.read_data_sets(DATA_DIR, one_hot=True)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_conv, labels=y_))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())    \n",
    "    \n",
    "    for i in range(STEPS):\n",
    "        batch = mnist.train.next_batch(50)\n",
    "        if i % 100 == 0:\n",
    "            train_accuracy = sess.run(accuracy, feed_dict={\n",
    "                x: batch[0],\n",
    "                y_: batch[1],\n",
    "                keep_prob: 1.0\n",
    "            })\n",
    "            print(\"Step {} accuracy {}\".format(i, train_accuracy))\n",
    "           \n",
    "        sess.run(train_step, feed_dict={\n",
    "            x: batch[0],\n",
    "            y_: batch[1],\n",
    "            keep_prob: 0.5\n",
    "        })\n",
    "        \n",
    "    X = mnist.test.images.reshape(10, 1000, 784)\n",
    "    Y = mnist.test.labels.reshape(10, 1000, 10)    \n",
    "    test_accuracy = np.mean([\n",
    "        sess.run(accuracy, feed_dict={x: X[i], y_: Y[i], keep_prob: 1.0})\n",
    "        for i in range(10)\n",
    "    ])\n",
    "    \n",
    "    print(\"Test accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train images: 50000\n",
      "Number of train labels: 50000\n",
      "Number of test images: 10000\n",
      "Number of test labels: 10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import _pickle\n",
    "# %matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DATA_PATH = './cifar-10-batches-py'\n",
    "        \n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(os.path.join(DATA_PATH, file), 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict        \n",
    "        \n",
    "def one_hot(vec, vals=10):\n",
    "    n = len(vec)\n",
    "    out = np.zeros((n, vals))\n",
    "    out[range(n), vec] = 1\n",
    "    return out\n",
    "\n",
    "class CifarLoader(object):\n",
    "    def __init__(self, source_files):\n",
    "        self._source = source_files\n",
    "        self._i = 0\n",
    "        self.images = None\n",
    "        self.labels = None\n",
    "        \n",
    "    def load(self):\n",
    "        data = [unpickle(f) for f in self._source]\n",
    "        images = np.vstack([d[b'data'] for d in data])\n",
    "        n = len(images)\n",
    "        self.images = images.reshape(n, 3, 32, 32).transpose(0, 2, 3, 1).astype(float) / 255\n",
    "        self.labels = one_hot(np.hstack(d[b'labels'] for d in data), 10)\n",
    "        return self\n",
    "    \n",
    "    def next_batch(self, batch_size):\n",
    "        x = self.images[self._i:self._i+batch_size]\n",
    "        y = self.labels[self._i:self._i+batch_size]\n",
    "        self._i = (self._i + batch_size) % len(self.images)\n",
    "        return x, y\n",
    "\n",
    "class CifarDataManager(object):\n",
    "    def __init__(self):\n",
    "        self.train = CifarLoader([\n",
    "            \"data_batch_{}\".format(i)\n",
    "            for i in range(1, 6)\n",
    "        ]).load()        \n",
    "        self.test = CifarLoader([\"test_batch\"]).load()\n",
    "        \n",
    "def display_cifar(images, size):\n",
    "    n = len(images)\n",
    "    plt.figure()\n",
    "    plt.gca().set_axis_off()\n",
    "    im = np.vstack([np.hstack([images[np.random.choice(n)] for i in range(size)]) for i in range(size)])\n",
    "    plt.imshow(im)\n",
    "    plt.show()\n",
    "    \n",
    "d = CifarDataManager()\n",
    "print(\"Number of train images: {}\".format(len(d.train.images)))\n",
    "print(\"Number of train labels: {}\".format(len(d.train.labels)))\n",
    "print(\"Number of test images: {}\".format(len(d.test.images)))\n",
    "print(\"Number of test labels: {}\".format(len(d.test.labels)))\n",
    "\n",
    "images = d.train.images\n",
    "display_cifar(images, 10)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-00cd32528abc>:30: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Step 0 accuracy 0.11999999731779099\n",
      "Step 100 accuracy 0.3799999952316284\n",
      "Step 200 accuracy 0.41999998688697815\n",
      "Step 300 accuracy 0.3400000035762787\n",
      "Step 400 accuracy 0.4000000059604645\n",
      "Step 500 accuracy 0.5199999809265137\n",
      "Step 600 accuracy 0.5199999809265137\n",
      "Step 700 accuracy 0.5199999809265137\n",
      "Step 800 accuracy 0.6600000262260437\n",
      "Step 900 accuracy 0.699999988079071\n",
      "Step 1000 accuracy 0.6000000238418579\n",
      "Step 1100 accuracy 0.5\n",
      "Step 1200 accuracy 0.5400000214576721\n",
      "Step 1300 accuracy 0.6200000047683716\n",
      "Step 1400 accuracy 0.4399999976158142\n",
      "Step 1500 accuracy 0.6399999856948853\n",
      "Step 1600 accuracy 0.6800000071525574\n",
      "Step 1700 accuracy 0.6200000047683716\n",
      "Step 1800 accuracy 0.6600000262260437\n",
      "Step 1900 accuracy 0.8399999737739563\n",
      "Step 2000 accuracy 0.6600000262260437\n",
      "Step 2100 accuracy 0.6399999856948853\n",
      "Step 2200 accuracy 0.7200000286102295\n",
      "Step 2300 accuracy 0.6600000262260437\n",
      "Step 2400 accuracy 0.5400000214576721\n",
      "Step 2500 accuracy 0.7400000095367432\n",
      "Step 2600 accuracy 0.7799999713897705\n",
      "Step 2700 accuracy 0.6800000071525574\n",
      "Step 2800 accuracy 0.7799999713897705\n",
      "Step 2900 accuracy 0.8999999761581421\n",
      "Step 3000 accuracy 0.7599999904632568\n",
      "Step 3100 accuracy 0.7200000286102295\n",
      "Step 3200 accuracy 0.7400000095367432\n",
      "Step 3300 accuracy 0.7400000095367432\n",
      "Step 3400 accuracy 0.6399999856948853\n",
      "Step 3500 accuracy 0.800000011920929\n",
      "Step 3600 accuracy 0.7400000095367432\n",
      "Step 3700 accuracy 0.7799999713897705\n",
      "Step 3800 accuracy 0.800000011920929\n",
      "Step 3900 accuracy 0.8600000143051147\n",
      "Step 4000 accuracy 0.8399999737739563\n",
      "Step 4100 accuracy 0.7400000095367432\n",
      "Step 4200 accuracy 0.7599999904632568\n",
      "Step 4300 accuracy 0.699999988079071\n",
      "Step 4400 accuracy 0.699999988079071\n",
      "Step 4500 accuracy 0.800000011920929\n",
      "Step 4600 accuracy 0.7599999904632568\n",
      "Step 4700 accuracy 0.7400000095367432\n",
      "Step 4800 accuracy 0.8399999737739563\n",
      "Step 4900 accuracy 0.8999999761581421\n",
      "Test accuracy: 70.75%\n"
     ]
    }
   ],
   "source": [
    "cifar = CifarDataManager()\n",
    "\n",
    "DATA_DIR = '/tmp/data'\n",
    "STEPS = 5000\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "C1, C2, C3 = 30, 50, 80\n",
    "F1 = 500\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "conv1_1 = conv_layer(x, shape=[3, 3, 3, C1])\n",
    "conv1_pool = max_pool_2x2(conv1_1)\n",
    "\n",
    "conv2_1 = conv_layer(conv1_pool, shape=[3, 3, C1, C2])\n",
    "conv2_pool = max_pool_2x2(conv2_1)\n",
    "\n",
    "conv3_1 = conv_layer(conv2_pool, shape=[3, 3, C2, C3])\n",
    "conv3_pool = max_pool_2x2(conv3_1)\n",
    "conv3_flat = tf.reshape(conv3_pool, [-1, 4*4*C3])\n",
    "conv3_drop = tf.nn.dropout(conv3_flat, keep_prob=keep_prob)\n",
    "\n",
    "full_1 = tf.nn.relu(full_layer(conv3_flat, F1))\n",
    "full1_drop = tf.nn.dropout(full_1, keep_prob=keep_prob)\n",
    "\n",
    "y_conv = full_layer(full1_drop, 10)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_conv, labels=y_))\n",
    "train_step = tf.train.AdamOptimizer(1e-3).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "def test(sess):\n",
    "    X = cifar.test.images.reshape(10, 1000, 32, 32, 3)\n",
    "    Y = cifar.test.labels.reshape(10, 1000, 10)\n",
    "    acc = np.mean([\n",
    "        sess.run(accuracy, feed_dict={x:X[i], y_:Y[i], keep_prob: 1.0}) \n",
    "        for i in range(10)\n",
    "    ])\n",
    "    print(\"Test accuracy: {:.4}%\".format(acc * 100))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())    \n",
    "    \n",
    "    for i in range(STEPS):\n",
    "        batch = cifar.train.next_batch(BATCH_SIZE)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            train_accuracy = sess.run(accuracy, feed_dict={\n",
    "                x: batch[0],\n",
    "                y_: batch[1],\n",
    "                keep_prob: 1.0\n",
    "            })\n",
    "            print(\"Step {} accuracy {}\".format(i, train_accuracy))\n",
    "            \n",
    "        sess.run(train_step, feed_dict={\n",
    "            x: batch[0],\n",
    "            y_: batch[1],\n",
    "            keep_prob: 0.5\n",
    "        })\n",
    "        \n",
    "    test(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([b'T', b'Te', b'Ten', b'Tens', b'Tenso', b'Tensor', b'Tensor ',\n",
       "       b'Tensor F', b'Tensor Fl', b'Tensor Flo', b'Tensor Flow'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elems = np.array(['T','e','n','s','o','r',' ','F','l','o','w'])\n",
    "scan_sum = tf.scan(lambda a, x: a + x, elems)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(scan_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN for MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-3d3c5a09bd44>:4: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From <ipython-input-1-3d3c5a09bd44>:79: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Iter 0, Minibatch loss= 2.302967, Training accuracy= 10.15625\n",
      "Iter 1000, Minibatch loss= 1.276900, Training accuracy= 54.68750\n",
      "Iter 2000, Minibatch loss= 0.697088, Training accuracy= 76.56250\n",
      "Iter 3000, Minibatch loss= 0.446154, Training accuracy= 85.93750\n",
      "Iter 4000, Minibatch loss= 0.121886, Training accuracy= 98.43750\n",
      "Iter 5000, Minibatch loss= 0.109163, Training accuracy= 97.65625\n",
      "Iter 6000, Minibatch loss= 0.089136, Training accuracy= 97.65625\n",
      "Iter 7000, Minibatch loss= 0.185452, Training accuracy= 96.09375\n",
      "Iter 8000, Minibatch loss= 0.147291, Training accuracy= 97.65625\n",
      "Iter 9000, Minibatch loss= 0.070955, Training accuracy= 99.21875\n",
      "Test accuracy: 99.21875\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets('/tmp/data', one_hot=True)\n",
    "\n",
    "element_size = 28\n",
    "time_steps = 28\n",
    "num_classes = 10\n",
    "batch_size = 128\n",
    "hidden_layer_size = 128\n",
    "\n",
    "LOG_DIR = 'logs/RNN_with_summaries'\n",
    "\n",
    "_inputs = tf.placeholder(tf.float32, shape=[None, time_steps, element_size], name='inputs')\n",
    "y = tf.placeholder(tf.float32, shape=[None, num_classes], name='labels')\n",
    "\n",
    "def variable_summaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)\n",
    "                             \n",
    "with tf.name_scope('rnn_weights'):\n",
    "    Wx = tf.Variable(tf.zeros([element_size, hidden_layer_size]))\n",
    "    variable_summaries(Wx)\n",
    "     \n",
    "with tf.name_scope('rnn_weights'):\n",
    "    with tf.name_scope('W_x'):\n",
    "        Wx = tf.Variable(tf.zeros([element_size, hidden_layer_size]))\n",
    "        variable_summaries(Wx)\n",
    "    with tf.name_scope('W_h'):\n",
    "        Wh = tf.Variable(tf.zeros([hidden_layer_size, hidden_layer_size]))\n",
    "        variable_summaries(Wh)\n",
    "    with tf.name_scope('Bias'):\n",
    "        b_rnn = tf.Variable(tf.zeros([hidden_layer_size]))\n",
    "        variable_summaries(b_rnn)\n",
    "                         \n",
    "def rnn_step(previous_hidden_state, x):\n",
    "    current_hidden_state = tf.tanh(\n",
    "        tf.matmul(previous_hidden_state, Wh) +\n",
    "        tf.matmul(x, Wx) + b_rnn\n",
    "    )\n",
    "    return current_hidden_state\n",
    "\n",
    "processed_inputs = tf.transpose(_inputs, perm=[1,0,2])\n",
    "initial_hidden = tf.zeros([batch_size, hidden_layer_size])\n",
    "all_hidden_states = tf.scan(rnn_step, processed_inputs, initializer=initial_hidden, name='states')\n",
    "                             \n",
    "with tf.name_scope('linear_layer_weights') as scope:\n",
    "    with tf.name_scope('W_linear'):\n",
    "        Wl = tf.Variable(tf.truncated_normal([\n",
    "            hidden_layer_size,\n",
    "            num_classes\n",
    "        ], mean=0, stddev=.01))\n",
    "        variable_summaries(Wl)\n",
    "                             \n",
    "    with tf.name_scope('Bias_linear'):\n",
    "        bl = tf.Variable(tf.truncated_normal(\n",
    "            [num_classes],\n",
    "            mean=0, stddev=.01\n",
    "        ))\n",
    "        variable_summaries(bl)\n",
    "\n",
    "def get_linear_layer(hidden_state):\n",
    "    return tf.matmul(hidden_state, Wl) + bl\n",
    "\n",
    "with tf.name_scope('linear_layer_weights') as scope:\n",
    "    all_outputs = tf.map_fn(get_linear_layer, all_hidden_states)\n",
    "    output = all_outputs[-1]\n",
    "    tf.summary.histogram('outputs', output)\n",
    "\n",
    "with tf.name_scope('cross_entropy') as scope:\n",
    "    cross_entropy = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=y)\n",
    "    )\n",
    "    tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "                             \n",
    "with tf.name_scope('train') as scope:\n",
    "    train_step = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cross_entropy)\n",
    "    \n",
    "with tf.name_scope('accuracy') as scope:\n",
    "    correct_prediction = tf.equal(\n",
    "        tf.argmax(y, 1),\n",
    "        tf.argmax(output, 1)\n",
    "    )\n",
    "    accuracy = (tf.reduce_mean(tf.cast(correct_prediction, tf.float32))) * 100\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "                             \n",
    "merged = tf.summary.merge_all()\n",
    "                             \n",
    "test_data = mnist.test.images[:batch_size].reshape((-1, time_steps, element_size))\n",
    "test_labels = mnist.test.labels[:batch_size]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    train_writer = tf.summary.FileWriter(LOG_DIR + '/train', graph=tf.get_default_graph())\n",
    "    test_writer = tf.summary.FileWriter(LOG_DIR + '/test', graph=tf.get_default_graph())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "                             \n",
    "    for i in range(10000):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        batch_x = batch_x.reshape((batch_size, time_steps, element_size))\n",
    "        summary, _ = sess.run([merged, train_step], feed_dict={_inputs:batch_x, y:batch_y})\n",
    "        train_writer.add_summary(summary, i)\n",
    "        if i % 1000 == 0:\n",
    "            acc, loss = sess.run([accuracy, cross_entropy], feed_dict={_inputs: batch_x, y: batch_y})\n",
    "                             \n",
    "            print(\n",
    "                'Iter ' + str(i) + ', Minibatch loss= ' + \\\n",
    "                '{:.6f}'.format(loss) + ', Training accuracy= ' + \\\n",
    "                '{:.5f}'.format(acc)\n",
    "            )\n",
    "                             \n",
    "        if i % 10 == 0:\n",
    "            summary, acc = sess.run([merged, accuracy], feed_dict={_inputs: test_data, y: test_labels})\n",
    "            test_writer.add_summary(summary, i)\n",
    "                             \n",
    "    test_acc = sess.run(accuracy, feed_dict={_inputs: test_data, y: test_labels})\n",
    "    print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
