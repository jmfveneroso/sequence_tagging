{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import functools\n",
    "from pathlib import Path\n",
    "import json\n",
    "import math\n",
    "from model.metrics import precision, recall, f1\n",
    "from model.cnn import masked_conv1d_and_max\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "DATADIR = 'data/conll2003'\n",
    "# DATADIR = 'data'\n",
    "\n",
    "# Params\n",
    "params = {\n",
    "    'dim_chars': 100,\n",
    "    'dim': 300,\n",
    "    'dropout': 0.5,\n",
    "    'num_oov_buckets': 1,\n",
    "    'epochs': 25,\n",
    "    'batch_size': 20,\n",
    "    'buffer': 15000,\n",
    "    'filters': 50,\n",
    "    'kernel_size': 3,\n",
    "    'lstm_size': 100,\n",
    "    'words': str(Path(DATADIR, 'vocab.words.txt')),\n",
    "    'chars': str(Path(DATADIR, 'vocab.chars.txt')),\n",
    "    'tags': str(Path(DATADIR, 'vocab.tags.txt')),\n",
    "    'glove': str(Path(DATADIR, 'glove.npz'))\n",
    "}\n",
    "\n",
    "with Path(DATADIR, 'params.json').open('w') as f:\n",
    "  json.dump(params, f, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "MAX_TOKEN_LENGTH = 10\n",
    "MINIBATCH_SIZE = 10\n",
    "\n",
    "def get_sentences(filename):    \n",
    "  with Path(DATADIR, filename).open('r', encoding=\"utf-8\") as f:\n",
    "    sentences = f.read().strip().split('\\n\\n')\n",
    "    return [[t.split() for t in s.split('\\n')] for s in sentences if len(s) > 0] \n",
    "\n",
    "# By grouping same length sentences we can make better use of the GPU.\n",
    "def group_by_len(arr): \n",
    "  groups = {}\n",
    "  for i in range(len(arr)):\n",
    "    key = len(arr[i])\n",
    "    if not key in groups:\n",
    "      groups[key] = []  \n",
    "    groups[key].append(i) \n",
    "  return groups\n",
    "\n",
    "def get_minibatches(sentences):\n",
    "  groups = group_by_len(sentences)\n",
    "  batches = [] \n",
    "        \n",
    "  for key in groups:\n",
    "    group_size = len(groups[key])\n",
    "    bins = int(math.ceil(group_size / float(MINIBATCH_SIZE)))\n",
    "    bin_size = int(math.ceil(group_size / float(bins)))\n",
    "    \n",
    "    for i in range(0, group_size, MINIBATCH_SIZE):\n",
    "      bin = []\n",
    "      for id in groups[key][i:i+MINIBATCH_SIZE]:\n",
    "        bin.append(id)\n",
    "      batches.append((key, bin))\n",
    "  return batches\n",
    "\n",
    "def parse_fn(sentence, label_col=3):\n",
    "    # Encode in Bytes for Tensorflow.\n",
    "    words = [s[0] for s in sentence]\n",
    "    tags = [s[label_col].encode() for s in sentence]\n",
    "    \n",
    "    # Chars.\n",
    "    chars = [[c.encode() for c in w] for w in words]\n",
    "    lengths = [len(c) for c in chars]\n",
    "    max_len = max(lengths)\n",
    "    chars = [c + [b'<pad>'] * (max_len - l) for c, l in zip(chars, lengths)]\n",
    "    \n",
    "    words = [s[0].encode() for s in sentence]      \n",
    "    return ((words, len(words)), (chars, lengths)), tags\n",
    "    \n",
    "def generator_fn(filename, label_col=3):\n",
    "    sentences = get_sentences(filename)\n",
    "    for s in sentences:\n",
    "        yield parse_fn(s, label_col)\n",
    "            \n",
    "    # minibatches = get_minibatches(sentences)\n",
    "    # for size, minibatch in minibatches:\n",
    "    #     for id in minibatch:\n",
    "    #         yield parse_fn(sentences[id])\n",
    "            \n",
    "def input_fn(filename, params=None, shuffle_and_repeat=False):\n",
    "  params = params if params is not None else {}\n",
    "  shapes = (\n",
    "     (([None], ()),           # (words, nwords)\n",
    "     ([None, None], [None])), # (chars, nchars)  \n",
    "     [None]                   # tags\n",
    "  )\n",
    "\n",
    "  types = (\n",
    "    ((tf.string, tf.int32),\n",
    "    (tf.string, tf.int32)),  \n",
    "    tf.string\n",
    "  )\n",
    "\n",
    "  defaults = (\n",
    "    (('<pad>', 0),\n",
    "    ('<pad>', 0)), \n",
    "    'O'\n",
    "  )\n",
    "\n",
    "  dataset = tf.data.Dataset.from_generator(\n",
    "    functools.partial(generator_fn, filename, 3),\n",
    "    output_types=types, output_shapes=shapes\n",
    "  )\n",
    "\n",
    "  if shuffle_and_repeat:\n",
    "    dataset = dataset.shuffle(params['buffer']).repeat(params['epochs'])\n",
    "\n",
    "  dataset = dataset.padded_batch(params.get('batch_size', 20), shapes, defaults)\n",
    "  return dataset\n",
    "\n",
    "# Estimator, train and evaluate\n",
    "train_inpf = functools.partial(input_fn, 'train', params, shuffle_and_repeat=True)\n",
    "eval_inpf  = functools.partial(input_fn, 'valid')\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_vocabulary(filenames):\n",
    "    if not isinstance(filenames, list):\n",
    "        filenames = [filenames]\n",
    "        \n",
    "    words = []\n",
    "    for f in filenames:\n",
    "        words = words + [w[0] for s in get_sentences(f) for w in s]\n",
    "    \n",
    "    words = list(set(words))\n",
    "    \n",
    "    with Path(DATADIR, 'vocab.words.txt').open('w', encoding='utf8') as f:\n",
    "        for w in words:\n",
    "            f.write(w + '\\n')\n",
    "               \n",
    "extract_vocabulary(['train', 'valid', 'test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "from six.moves import reduce\n",
    "import tensorflow as tf\n",
    "\n",
    "def model_fn(features, labels, mode, params):\n",
    "    # For serving features are a bit different\n",
    "    if isinstance(features, dict):\n",
    "        features = ((features['words'], features['nwords']),\n",
    "                    (features['chars'], features['nchars']))\n",
    "\n",
    "    # Read vocabs and inputs\n",
    "    dropout = params['dropout']\n",
    "    (words, nwords), (chars, nchars) = features\n",
    "    training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    vocab_words = tf.contrib.lookup.index_table_from_file(\n",
    "        params['words'], num_oov_buckets=params['num_oov_buckets'])\n",
    "    vocab_chars = tf.contrib.lookup.index_table_from_file(\n",
    "        params['chars'], num_oov_buckets=params['num_oov_buckets'])\n",
    "    with Path(params['tags']).open() as f:\n",
    "        indices = [idx for idx, tag in enumerate(f) if tag.strip() != 'O']\n",
    "        num_tags = len(indices) + 1\n",
    "    with Path(params['chars']).open() as f:\n",
    "        num_chars = sum(1 for _ in f) + params['num_oov_buckets']\n",
    "\n",
    "    # Char Embeddings\n",
    "    char_ids = vocab_chars.lookup(chars)\n",
    "    variable = tf.get_variable(\n",
    "        'chars_embeddings', [num_chars + 1, params['dim_chars']], tf.float32)\n",
    "    char_embeddings = tf.nn.embedding_lookup(variable, char_ids)\n",
    "    char_embeddings = tf.layers.dropout(char_embeddings, rate=dropout,\n",
    "                                        training=training)\n",
    "\n",
    "    # Char 1d convolution\n",
    "    weights = tf.sequence_mask(nchars)\n",
    "    char_embeddings = masked_conv1d_and_max(\n",
    "        char_embeddings, weights, params['filters'], params['kernel_size'])\n",
    "\n",
    "    # Word Embeddings\n",
    "    word_ids = vocab_words.lookup(words)\n",
    "    glove = np.load(params['glove'])['embeddings']  # np.array\n",
    "    variable = np.vstack([glove, [[0.] * params['dim']]])\n",
    "    variable = tf.Variable(variable, dtype=tf.float32, trainable=False)\n",
    "    word_embeddings = tf.nn.embedding_lookup(variable, word_ids)\n",
    "\n",
    "    # Concatenate Word and Char Embeddings\n",
    "    embeddings = tf.concat([word_embeddings, char_embeddings], axis=-1)\n",
    "    embeddings = tf.layers.dropout(embeddings, rate=dropout, training=training)\n",
    "\n",
    "    # LSTM\n",
    "    t = tf.transpose(embeddings, perm=[1, 0, 2])  # Need time-major\n",
    "    lstm_cell_fw = tf.contrib.rnn.LSTMBlockFusedCell(params['lstm_size'])\n",
    "    lstm_cell_bw = tf.contrib.rnn.LSTMBlockFusedCell(params['lstm_size'])\n",
    "    lstm_cell_bw = tf.contrib.rnn.TimeReversedFusedRNN(lstm_cell_bw)\n",
    "    output_fw, _ = lstm_cell_fw(t, dtype=tf.float32, sequence_length=nwords)\n",
    "    output_bw, _ = lstm_cell_bw(t, dtype=tf.float32, sequence_length=nwords)\n",
    "    output = tf.concat([output_fw, output_bw], axis=-1)\n",
    "    output = tf.transpose(output, perm=[1, 0, 2])\n",
    "    output = tf.layers.dropout(output, rate=dropout, training=training)\n",
    "\n",
    "    # CRF\n",
    "    logits = tf.layers.dense(output, num_tags)\n",
    "    crf_params = tf.get_variable(\"crf\", [num_tags, num_tags], dtype=tf.float32)\n",
    "    pred_ids, _ = tf.contrib.crf.crf_decode(logits, crf_params, nwords)\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        # Predictions\n",
    "        reverse_vocab_tags = tf.contrib.lookup.index_to_string_table_from_file(\n",
    "            params['tags'])\n",
    "        pred_strings = reverse_vocab_tags.lookup(tf.to_int64(pred_ids))\n",
    "        predictions = {\n",
    "            'pred_ids': pred_ids,\n",
    "            'tags': pred_strings\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "    else:\n",
    "        # Loss\n",
    "        vocab_tags = tf.contrib.lookup.index_table_from_file(params['tags'])\n",
    "        tags = vocab_tags.lookup(labels)\n",
    "        log_likelihood, _ = tf.contrib.crf.crf_log_likelihood(\n",
    "            logits, tags, nwords, crf_params)\n",
    "        loss = tf.reduce_mean(-log_likelihood)\n",
    "\n",
    "        # Metrics\n",
    "        weights = tf.sequence_mask(nwords)\n",
    "        metrics = {\n",
    "            'acc': tf.metrics.accuracy(tags, pred_ids, weights),\n",
    "            'precision': precision(tags, pred_ids, num_tags, indices, weights),\n",
    "            'recall': recall(tags, pred_ids, num_tags, indices, weights),\n",
    "            'f1': f1(tags, pred_ids, num_tags, indices, weights),\n",
    "        }\n",
    "        for metric_name, op in metrics.items():\n",
    "            tf.summary.scalar(metric_name, op[1])\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.EVAL:\n",
    "            return tf.estimator.EstimatorSpec(\n",
    "                mode, loss=loss, eval_metric_ops=metrics)\n",
    "\n",
    "        elif mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            train_op = tf.train.AdamOptimizer().minimize(\n",
    "                loss, global_step=tf.train.get_or_create_global_step())\n",
    "            return tf.estimator.EstimatorSpec(\n",
    "                mode, loss=loss, train_op=train_op)\n",
    "    \n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_tf_random_seed': None, '_model_dir': 'results/model', '_task_id': 0, '_num_worker_replicas': 1, '_task_type': 'worker', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fc5f14a86a0>, '_log_step_count_steps': 100, '_master': '', '_global_id_in_cluster': 0, '_is_chief': True, '_protocol': None, '_save_checkpoints_steps': None, '_train_distribute': None, '_save_checkpoints_secs': 120, '_experimental_distribute': None, '_num_ps_replicas': 0, '_eval_distribute': None, '_device_fn': None, '_evaluation_master': '', '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_keep_checkpoint_max': 5, '_save_summary_steps': 100}\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 120.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into results/model/model.ckpt.\n",
      "INFO:tensorflow:loss = 21.368576, step = 1\n",
      "INFO:tensorflow:global_step/sec: 4.26704\n",
      "INFO:tensorflow:loss = 3.4160933, step = 101 (23.437 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.27732\n",
      "INFO:tensorflow:loss = 4.503299, step = 201 (23.379 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.48604\n",
      "INFO:tensorflow:loss = 2.9426212, step = 301 (22.291 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.29252\n",
      "INFO:tensorflow:loss = 2.5728002, step = 401 (23.297 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.34569\n",
      "INFO:tensorflow:loss = 2.8045115, step = 501 (23.011 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 507 into results/model/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-13-00:20:36\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from results/model/model.ckpt-507\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [10/100]\n",
      "INFO:tensorflow:Evaluation [20/100]\n",
      "INFO:tensorflow:Evaluation [30/100]\n",
      "INFO:tensorflow:Evaluation [40/100]\n",
      "INFO:tensorflow:Evaluation [50/100]\n",
      "INFO:tensorflow:Evaluation [60/100]\n",
      "INFO:tensorflow:Evaluation [70/100]\n",
      "INFO:tensorflow:Evaluation [80/100]\n",
      "INFO:tensorflow:Evaluation [90/100]\n",
      "INFO:tensorflow:Evaluation [100/100]\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-13-00:20:43\n",
      "INFO:tensorflow:Saving dict for global step 507: acc = 0.97119385, f1 = 0.86819905, global_step = 507, loss = 1.3986866, precision = 0.877451, recall = 0.8591402\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 507: results/model/model.ckpt-507\n"
     ]
    }
   ],
   "source": [
    "cfg = tf.estimator.RunConfig(save_checkpoints_secs=120)\n",
    "estimator = tf.estimator.Estimator(model_fn, 'results/model', cfg, params)\n",
    "Path(estimator.eval_dir()).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Early stop if F1 does not increase.\n",
    "hook = tf.contrib.estimator.stop_if_no_increase_hook(estimator, 'f1', 500, min_steps=8000, run_every_secs=120)\n",
    "\n",
    "train_spec = tf.estimator.TrainSpec(input_fn=train_inpf, hooks=[hook])\n",
    "eval_spec = tf.estimator.EvalSpec(input_fn=eval_inpf, throttle_secs=120)\n",
    "tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write predictions to file.\n",
    "def write_predictions(name):\n",
    "  Path('results/score').mkdir(parents=True, exist_ok=True)\n",
    "  with Path('results/score/{}.preds.txt'.format(name)).open('wb') as f:\n",
    "    test_inpf = functools.partial(input_fn, name)\n",
    "    golds_gen = generator_fn(name)\n",
    "    preds_gen = estimator.predict(test_inpf)\n",
    "    for golds, preds in zip(golds_gen, preds_gen):     \n",
    "      ((words, _),(_, _)), tags = golds\n",
    "      for word, tag, tag_pred in zip(words, tags, preds['tags']):\n",
    "        f.write(b' '.join([word, tag, tag_pred]) + b'\\n')\n",
    "      f.write(b'\\n')\n",
    "\n",
    "for name in ['train', 'valid', 'test']:\n",
    "  write_predictions(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./conlleval < results/score/test.preds.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
