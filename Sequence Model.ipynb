{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "digit_to_word_map = {\n",
    "    0: 'pad', 1: 'one', 2: 'two', 3: 'three', 4: 'four', \n",
    "    5: 'five', 6: 'six', 7: 'seven', 8: 'eight', 9: 'nine'\n",
    "}\n",
    "\n",
    "even_sentences = []\n",
    "odd_sentences = []\n",
    "seqlens = []\n",
    "for i in range(10000):\n",
    "    rand_seq_len = np.random.choice(range(3,7))\n",
    "    seqlens.append(rand_seq_len)\n",
    "    rand_odd_ints = np.random.choice(range(1,10,2), rand_seq_len)\n",
    "    rand_even_ints = np.random.choice(range(2,10,2), rand_seq_len)    \n",
    "    \n",
    "    if rand_seq_len < 6:\n",
    "        rand_odd_ints = np.append(rand_odd_ints, [0]*(6-rand_seq_len))\n",
    "        rand_even_ints = np.append(rand_even_ints, [0]*(6-rand_seq_len))        \n",
    "        \n",
    "    even_sentences.append(' '.join([digit_to_word_map[r] for r in rand_even_ints]))\n",
    "    odd_sentences.append(' '.join([digit_to_word_map[r] for r in rand_odd_ints]))    \n",
    "    \n",
    "data = even_sentences + odd_sentences\n",
    "seqlens *= 2\n",
    "\n",
    "word2index_map = {}\n",
    "index = 0\n",
    "for sent in data:\n",
    "    for word in sent.lower().split():\n",
    "        if word not in word2index_map:\n",
    "            word2index_map[word] = index\n",
    "            index += 1\n",
    "\n",
    "index2word_map = {index: word for word, index in word2index_map.items()}\n",
    "vocabulary_size = len(index2word_map)\n",
    "\n",
    "labels = [1]*10000 + [0]*10000\n",
    "for i in range(len(labels)):\n",
    "    label = labels[i]\n",
    "    one_hot_encoding = [0]*2\n",
    "    one_hot_encoding[label] = 1\n",
    "    labels[i] = one_hot_encoding\n",
    "    \n",
    "data_indices = list(range(len(data)))\n",
    "np.random.shuffle(data_indices)\n",
    "data = np.array(data)[data_indices]\n",
    "\n",
    "labels = np.array(labels)[data_indices]\n",
    "seqlens = np.array(seqlens)[data_indices]\n",
    "\n",
    "train_x = data[:10000]\n",
    "train_y = labels[:10000]\n",
    "train_seqlens = seqlens[:10000]\n",
    "\n",
    "test_x = data[10000:]\n",
    "test_y = labels[10000:]\n",
    "test_seqlens = seqlens[10000:]\n",
    "\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "path_to_glove = 'data/glove.840B.300d.zip'\n",
    "PRE_TRAINED = True\n",
    "GLOVE_SIZE = 300\n",
    "batch_size = 128\n",
    "embedding_dimension = 64\n",
    "num_classes = 2\n",
    "hidden_layer_size = 32\n",
    "time_steps = 6\n",
    "\n",
    "def get_glove(path_to_glove, word2index_map):\n",
    "    embedding_weights = {}\n",
    "    count_all_words = 0\n",
    "    \n",
    "    with zipfile.ZipFile(path_to_glove) as z:\n",
    "        with z.open('glove.840B.300d.txt') as f:\n",
    "            for line in f:\n",
    "                vals = line.split()\n",
    "                word = str(vals[0].decode(\"utf-8\"))\n",
    "                \n",
    "                if word in word2index_map:\n",
    "                    count_all_words += 1\n",
    "                    coefs = np.asarray(vals[1:], dtype='float32')\n",
    "                    coefs /= np.linalg.norm(coefs)\n",
    "                    embedding_weights[word] = coefs\n",
    "                    \n",
    "                if count_all_words == vocabulary_size - 1:\n",
    "                    break   \n",
    "                    \n",
    "    return embedding_weights\n",
    "\n",
    "word2embedding_dict = get_glove(path_to_glove, word2index_map)\n",
    "embedding_matrix = np.zeros((vocabulary_size, GLOVE_SIZE))\n",
    "for word, index in word2index_map.items():\n",
    "    if not word == \"pad\":\n",
    "        word_embedding = word2embedding_dict[word]\n",
    "        embedding_matrix[index,:] = word_embedding\n",
    "        \n",
    "data_indices = list(range(len(data)))\n",
    "np.random.shuffle(data_indices)\n",
    "data = np.array(data)[data_indices]\n",
    "labels = np.array(labels)[data_indices]\n",
    "seqlens = np.array(seqlens)[data_indices]\n",
    "train_x = data[:10000]\n",
    "train_y = labels[:10000]\n",
    "train_seqlens = seqlens[:10000]\n",
    "test_x = data[10000:]\n",
    "test_y = labels[10000:]\n",
    "test_seqlens = seqlens[10000:]\n",
    "\n",
    "def get_sentence_batch(batch_size, data_x, data_y, data_seqlens):\n",
    "    instance_indices = list(range(len(data_x)))\n",
    "    np.random.shuffle(instance_indices)\n",
    "    batch = instance_indices[:batch_size]\n",
    "    x = [[word2index_map[word] for word in data_x[i].split()] for i in batch]\n",
    "    y = [data_y[i] for i in batch]\n",
    "    seqlens = [data_seqlens[i] for i in batch]\n",
    "    return x, y, seqlens\n",
    "\n",
    "# print(even_sentences[0:6])\n",
    "# print(odd_sentences[0:6])\n",
    "# print(seqlens[0:6])\n",
    "\n",
    "x, y, seqlens = get_sentence_batch(20, train_x, train_y, train_seqlens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Cross Entropy: 0.690684, Accuracy: 0.650000\n",
      "Train Cross Entropy: 0.121787, Accuracy: 1.000000\n",
      "Train Cross Entropy: 0.009364, Accuracy: 1.000000\n",
      "Train Cross Entropy: 0.005921, Accuracy: 1.000000\n",
      "Train Cross Entropy: 0.002550, Accuracy: 1.000000\n",
      "Test Cross Entropy: 0.001505, Accuracy: 1.000000\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 20\n",
    "lstm_size = 10\n",
    "\n",
    "with tf.name_scope('input'):\n",
    "    x = tf.placeholder(tf.int32, shape=(None, 6))\n",
    "    y = tf.placeholder(tf.int32, shape=(None, 2))\n",
    "    seqlens = tf.placeholder(tf.int32, shape=(None,))    \n",
    "    emb = tf.nn.embedding_lookup(embedding_matrix, x)\n",
    "    \n",
    "with tf.name_scope('lstm'):\n",
    "    cells = [tf.nn.rnn_cell.LSTMCell(lstm_size) for _ in range(2)]\n",
    "    lstm_cell = tf.nn.rnn_cell.MultiRNNCell(cells)\n",
    "    \n",
    "    outputs, (cell_state, states) = tf.nn.dynamic_rnn(\n",
    "        lstm_cell, emb, sequence_length=seqlens, \n",
    "        dtype=tf.float64\n",
    "    )\n",
    "    state = states[-1]\n",
    "    \n",
    "with tf.name_scope('output'):\n",
    "    logits = tf.layers.dense(state, 2)\n",
    "    \n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        labels=y,\n",
    "        logits=logits,\n",
    "        name='loss'\n",
    "    ))\n",
    "    correct = tf.equal(\n",
    "        tf.argmax(y, 1), \n",
    "        tf.argmax(logits, 1)\n",
    "    )\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))    \n",
    "    train_step = tf.train.AdamOptimizer(0.001).minimize(cross_entropy)\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(500):\n",
    "        batch_x, batch_y, batch_seqlens = get_sentence_batch(\n",
    "            batch_size, train_x, train_y, train_seqlens\n",
    "        )\n",
    "        \n",
    "        sess.run(train_step, {\n",
    "            x: batch_x,\n",
    "            y: batch_y,            \n",
    "            seqlens: batch_seqlens\n",
    "        })\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            ce, acc = sess.run([cross_entropy, accuracy], {\n",
    "                x: batch_x,\n",
    "                y: batch_y,            \n",
    "                seqlens: batch_seqlens\n",
    "            })\n",
    "            print('Train Cross Entropy: %f, Accuracy: %f' % (ce, acc))\n",
    "        \n",
    "    test_x, test_y, test_seqlens = get_sentence_batch(100000, test_x, test_y, test_seqlens)\n",
    "        \n",
    "    ce, acc = sess.run([cross_entropy, accuracy], {\n",
    "        x: test_x,\n",
    "        y: test_y,            \n",
    "        seqlens: test_seqlens\n",
    "    })\n",
    "    print('Test Cross Entropy: %f, Accuracy: %f' % (ce, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
