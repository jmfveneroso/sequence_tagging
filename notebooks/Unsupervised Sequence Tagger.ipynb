{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gazetteer Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import itertools\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "def get_tokens(filename):\n",
    "    tkns = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            tkns += line.strip().lower().split()\n",
    "    return tkns       \n",
    "    \n",
    "def get_word_counts(filename):\n",
    "    tkns = get_tokens(filename)\n",
    "    return Counter(tkns)\n",
    "    \n",
    "def plot_word_frequencies(filename):\n",
    "    counter = get_word_counts(filename)\n",
    "    name_counts = Counter([counter[key] for key in counter])\n",
    "    samples = []\n",
    "    \n",
    "    for i in range(1, 500):\n",
    "        if i in name_counts:\n",
    "            samples.append(np.log(name_counts[i]))\n",
    "        else:\n",
    "            samples.append(0)\n",
    "        \n",
    "    return samples\n",
    "\n",
    "word_count = get_word_counts('../data/words.txt')\n",
    "name_count = get_word_counts('../data/names.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 32.4179104\n",
    "\n",
    "priors = {}\n",
    "\n",
    "with open('../data/ner_on_html/vocab.words.txt') as f:\n",
    "    for n in f:\n",
    "        n = n.strip().lower()\n",
    "        a = name_count[n]\n",
    "        b = round(word_count[n] * k)\n",
    "        b = 1 if a > b else b - a\n",
    "        priors[n] = (a, b)\n",
    "        \n",
    "# Scale priors.\n",
    "for w in priors: \n",
    "    k = 0.01\n",
    "    priors[w] = (round(k * priors[w][0])+1, round(k * priors[w][1])+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_est(k, n, a, b):\n",
    "    return (k + a - 1) / (n + a + b - 2)\n",
    "\n",
    "counts = {}\n",
    "for p in priors:\n",
    "    counts[p] = [0, 0]\n",
    "\n",
    "probs = {}\n",
    "def update_probs():\n",
    "    global probs\n",
    "    for p in priors:\n",
    "        k, n = counts[p]\n",
    "        alpha, beta = priors[p]\n",
    "        probs[p] = map_est(k, n, 1+alpha, 1+beta)\n",
    "update_probs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.821717194559\n",
      "Acc: 0.838790433871\n",
      "Acc: 0.841318442645\n",
      "Acc: 0.841968317872\n",
      "Acc: 0.84158705471\n",
      "Acc: 0.841719799912\n",
      "Acc: 0.841105207079\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-0b57bbb0fb20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwords_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0mlabel_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlabels_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                 \u001b[0mtrain_label_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m             })\n\u001b[1;32m     87\u001b[0m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1290\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1275\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1277\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1365\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "\n",
    "def get_sentences(f):\n",
    "    with open(f, 'r', encoding=\"utf-8\") as f:\n",
    "        sentences = f.read().strip().split('\\n\\n')\n",
    "        sentences = [[t.split() for t in s.split('\\n')] for s in sentences if len(s) > 0]\n",
    "        return sentences\n",
    "    \n",
    "sentences = get_sentences('../data/ner_on_html/test')\n",
    "sentences = [[(t[0], 1 if t[1] != 'O' else 0) for t in s] for s in sentences if s[0][0] != '-DOCSTART-']\n",
    "\n",
    "def get_random_labels(sentence):\n",
    "    labels = []\n",
    "    for t in sentence:        \n",
    "        p = probs[t]\n",
    "        y = np.random.choice(a=[0, 1], p=[1-p, p])\n",
    "        labels.append(y)  \n",
    "    return labels\n",
    "\n",
    "words = tf.placeholder(tf.string, shape=(None,), name='words')\n",
    "label_ids = tf.placeholder(tf.int32, shape=(None,), name='label_ids')\n",
    "train_label_ids = tf.placeholder(tf.int32, shape=(None,), name='train_label_ids')\n",
    "\n",
    "labels = tf.one_hot(train_label_ids, 2)\n",
    "\n",
    "vocab_words = tf.contrib.lookup.index_table_from_file(\n",
    "    '../data/ner_on_html/vocab.words.txt', num_oov_buckets=1\n",
    ")\n",
    "\n",
    "word_ids = vocab_words.lookup(words)\n",
    "glove = np.load('../data/ner_on_html/glove.npz')['embeddings']\n",
    "variable = np.vstack([glove, [[0.] * 300]])\n",
    "variable = tf.Variable(variable, dtype=tf.float32, trainable=False)\n",
    "word_embs = tf.nn.embedding_lookup(variable, word_ids)\n",
    "\n",
    "with tf.variable_scope('lstm', reuse=tf.AUTO_REUSE):\n",
    "    lstm_cell_fw = tf.nn.rnn_cell.LSTMCell(100)\n",
    "    lstm_cell_bw = tf.nn.rnn_cell.LSTMCell(100)\n",
    "\n",
    "    word_embs = tf.reshape(word_embs, [1, -1, 300])\n",
    "    (output_fw, output_bw), (_, _) = tf.nn.bidirectional_dynamic_rnn(\n",
    "        lstm_cell_fw, lstm_cell_bw, word_embs,\n",
    "        dtype=tf.float32\n",
    "    )\n",
    "\n",
    "    output = tf.concat([output_fw, output_bw], axis=-1)\n",
    "    output = tf.reshape(output, [-1, 200])\n",
    "    \n",
    "    logits = tf.layers.dense(output, 2)\n",
    "    \n",
    "    # Uncomment to predict based on priors only.\n",
    "    # pred_ids = tf.argmax(labels, axis=-1)    \n",
    "    pred_ids = tf.argmax(logits, axis=-1)\n",
    "    correct = tf.equal(tf.to_int32(pred_ids), label_ids)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "        \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=logits))\n",
    "    train_step = tf.train.AdamOptimizer().minimize(loss)\n",
    "    \n",
    "counts = {}\n",
    "for p in priors:\n",
    "    counts[p] = [0, 0]    \n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    tf.tables_initializer().run()\n",
    "    tf.initializers.global_variables().run()\n",
    "    \n",
    "    for _ in range(30):\n",
    "        new_counts = {}\n",
    "        for p in priors:\n",
    "            new_counts[p] = [0, 0]\n",
    "        \n",
    "        a = []\n",
    "                \n",
    "        shuffle(sentences)\n",
    "        for i, s in enumerate(sentences):\n",
    "            words_ = [t[0].lower() for t in s]\n",
    "            labels_ = [t[1] for t in s]\n",
    "            train_labels = get_random_labels(words_)\n",
    "            acc, preds, _ = sess.run([accuracy, pred_ids, train_step], feed_dict={\n",
    "                words: words_,\n",
    "                label_ids: labels_,\n",
    "                train_label_ids: train_labels\n",
    "            })\n",
    "            a.append(acc)\n",
    "        \n",
    "            for j, p in enumerate(preds):\n",
    "                if p == 1:\n",
    "                    new_counts[words_[j]][0] += 5\n",
    "                new_counts[words_[j]][1] += 5\n",
    "        \n",
    "        print('Acc:', sum(a)/float(len(a)))\n",
    "        counts = new_counts\n",
    "        update_probs()\n",
    "\n",
    "print('All other: 0.89')\n",
    "print('All prior: 0.8054')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 8)\n",
      "[20, 30]\n",
      "0.7\n"
     ]
    }
   ],
   "source": [
    "n = 'ronald'\n",
    "\n",
    "print(priors[n])\n",
    "print(counts[n])\n",
    "\n",
    "print((priors[n][0]+priors[n][1])/counts[n][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1.  1.  1.]\n",
      "  [ 1.  1.  1.]\n",
      "  [ 1.  1.  1.]]\n",
      "\n",
      " [[ 1.  0.  0.]\n",
      "  [ 0.  1.  0.]\n",
      "  [ 0.  0.  1.]]]\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([\n",
    "    [\n",
    "        [1, 2, 3],\n",
    "        [1, 2, 3],\n",
    "        [1, 2, 3],\n",
    "    ],\n",
    "    [\n",
    "        [1, 2, 3],\n",
    "        [4, 5, 6],\n",
    "        [6, 7, 9],\n",
    "    ]\n",
    "], dtype=tf.int64)\n",
    "\n",
    "Q = tf.transpose(x, [1, 0, 2]) \n",
    "K = tf.transpose(x, [1, 0, 2]) \n",
    "\n",
    "A = tf.map_fn(\n",
    "    lambda k: tf.cast(tf.reduce_all(tf.equal(Q, k), axis=-1), tf.int64),\n",
    "    K\n",
    ")\n",
    "\n",
    "z = tf.cast(tf.transpose(A, [2, 1, 0]), tf.float32)\n",
    "# z = tf.cast(tf.reduce_sum(tf.cast(tf.equal(x, y), tf.int32), axis=-1), tf.float32)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run([tf.initializers.global_variables(), tf.tables_initializer()])\n",
    "    res = sess.run(z)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
